{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dewwbe/-Real-Estate-Document-Collection-/blob/main/IT22910790.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13654dd1",
      "metadata": {
        "id": "13654dd1"
      },
      "source": [
        "\n",
        "# LSTM — IMDB Sentiment Analysis (IT22910790)\n",
        "\n",
        "**Objective:** Implement an LSTM model for sentiment analysis on the IMDB movie reviews dataset and compare **Bidirectional vs Unidirectional** LSTM.\n",
        "\n",
        "## What to do\n",
        "1. Upload `IMDB Dataset.csv` to the notebook's root/content directory.  \n",
        "2. This notebook is named `IT22910790Q3.ipynb`.  \n",
        "3. Modify **Embedding `output_dim`**, **LSTM `units`**, **dropout**, **epochs**, and **batch_size** to see effects on performance.  \n",
        "4. Train and record **Accuracy** and **F1-score**.  \n",
        "5. Compare **Bidirectional LSTM** vs **Unidirectional LSTM** (both implemented below).  \n",
        "6. If scores are low, try different architectures and hyperparameters.  \n",
        "7. Write your analysis in the **Observations & Comparison** section at the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f24d0c04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f24d0c04",
        "outputId": "63678765-6e60-44bb-b761-9e46520fb80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === Setup (Colab users: uncomment the next line if needed) ===\n",
        "# !pip install -q tensorflow pandas scikit-learn matplotlib\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d3f73ebe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3f73ebe",
        "outputId": "fa8a3637-04da-4f1f-cd04-7dfe6da8b45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (50000, 2)\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "\n",
            "Class balance:\n",
            " label\n",
            "1    25000\n",
            "0    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === Load IMDB dataset ===\n",
        "# Expected columns: 'review' and 'sentiment' with values 'positive' or 'negative'\n",
        "df = pd.read_csv(\"/content/IMDB Dataset.csv\")\n",
        "print(\"Data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Basic cleaning: remove HTML tags, lowercase\n",
        "def clean_text(t):\n",
        "    t = re.sub(r\"<.*?>\", \" \", str(t))  # remove HTML tags\n",
        "    t = re.sub(r\"[^a-zA-Z0-9' ]\", \" \", t)  # keep letters, numbers, apostrophes, spaces\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t.lower()\n",
        "\n",
        "df[\"review_clean\"] = df[\"review\"].apply(clean_text)\n",
        "df[\"label\"] = (df[\"sentiment\"].str.lower().map({\"positive\":1, \"negative\":0})).astype(int)\n",
        "\n",
        "print(\"\\nClass balance:\\n\", df[\"label\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2d7c3724",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d7c3724",
        "outputId": "7c25c5de-9cc8-4579-be2e-3543eeb8d127"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((40000, 200), (10000, 200))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "# === Tokenize & Pad ===\n",
        "VOCAB_SIZE = 20000  # changeable\n",
        "MAXLEN = 200        # changeable\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df[\"review_clean\"])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df[\"review_clean\"])\n",
        "X = pad_sequences(sequences, maxlen=MAXLEN, padding=\"post\", truncating=\"post\")\n",
        "y = df[\"label\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train.shape, X_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1fdc67e5",
      "metadata": {
        "id": "1fdc67e5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Hyperparameters (Tweak Here) ===\n",
        "EMBED_DIM = 64    # e.g., 32, 64, 128\n",
        "LSTM_UNITS = 64   # e.g., 32, 64, 128\n",
        "DROPOUT = 0.3     # e.g., 0.2, 0.3, 0.5\n",
        "EPOCHS = 3        # e.g., 3, 5, 10\n",
        "BATCH_SIZE = 64   # e.g., 32, 64, 128\n",
        "\n",
        "def make_model(bidirectional=True):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM, input_length=MAXLEN))\n",
        "    if bidirectional:\n",
        "        model.add(Bidirectional(LSTM(LSTM_UNITS)))\n",
        "    else:\n",
        "        model.add(LSTM(LSTM_UNITS))\n",
        "    model.add(Dropout(DROPOUT))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "91f3624d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91f3624d",
        "outputId": "40e9730f-e465-4a67-b662-93c029927b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 297ms/step - accuracy: 0.6548 - loss: 0.5845 - val_accuracy: 0.8690 - val_loss: 0.3200\n",
            "Epoch 2/3\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 299ms/step - accuracy: 0.8976 - loss: 0.2721 - val_accuracy: 0.8487 - val_loss: 0.3812\n",
            "Epoch 3/3\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 295ms/step - accuracy: 0.9328 - loss: 0.1883 - val_accuracy: 0.8593 - val_loss: 0.3493\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 51ms/step\n",
            "Bidirectional LSTM — Accuracy: 0.8605, F1: 0.8708\n",
            "\n",
            "Classification Report (Bidirectional):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.93      0.78      0.85      5000\n",
            "    positive       0.81      0.94      0.87      5000\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.87      0.86      0.86     10000\n",
            "weighted avg       0.87      0.86      0.86     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === Train Bidirectional LSTM ===\n",
        "bi_model = make_model(bidirectional=True)\n",
        "history_bi = bi_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_bi_proba = bi_model.predict(X_test).ravel()\n",
        "y_pred_bi = (y_pred_bi_proba >= 0.5).astype(int)\n",
        "\n",
        "bi_acc = accuracy_score(y_test, y_pred_bi)\n",
        "bi_f1 = f1_score(y_test, y_pred_bi)\n",
        "\n",
        "print(f\"Bidirectional LSTM — Accuracy: {bi_acc:.4f}, F1: {bi_f1:.4f}\")\n",
        "print(\"\\nClassification Report (Bidirectional):\\n\", classification_report(y_test, y_pred_bi, target_names=[\"negative\",\"positive\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090b55a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "090b55a6",
        "outputId": "abe6b04c-3fd1-41ff-de67-540d9d2ab709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m557/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - accuracy: 0.5256 - loss: 0.6898"
          ]
        }
      ],
      "source": [
        "\n",
        "# === Train Unidirectional LSTM ===\n",
        "uni_model = make_model(bidirectional=False)\n",
        "history_uni = uni_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_uni_proba = uni_model.predict(X_test).ravel()\n",
        "y_pred_uni = (y_pred_uni_proba >= 0.5).astype(int)\n",
        "\n",
        "uni_acc = accuracy_score(y_test, y_pred_uni)\n",
        "uni_f1 = f1_score(y_test, y_pred_uni)\n",
        "\n",
        "print(f\"Unidirectional LSTM — Accuracy: {uni_acc:.4f}, F1: {uni_f1:.4f}\")\n",
        "print(\"\\nClassification Report (Unidirectional):\\n\", classification_report(y_test, y_pred_uni, target_names=[\"negative\",\"positive\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d59bb8e0",
      "metadata": {
        "id": "d59bb8e0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Plot training curves for quick visual comparison ===\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(history_bi.history[\"val_accuracy\"], label=\"BiLSTM val_acc\")\n",
        "plt.plot(history_uni.history[\"val_accuracy\"], label=\"UniLSTM val_acc\", linestyle=\"--\")\n",
        "plt.title(\"Validation Accuracy: BiLSTM vs UniLSTM\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(history_bi.history[\"val_loss\"], label=\"BiLSTM val_loss\")\n",
        "plt.plot(history_uni.history[\"val_loss\"], label=\"UniLSTM val_loss\", linestyle=\"--\")\n",
        "plt.title(\"Validation Loss: BiLSTM vs UniLSTM\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9011084a",
      "metadata": {
        "id": "9011084a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Summary Table ===\n",
        "summary = pd.DataFrame({\n",
        "    \"Model\": [\"Bidirectional LSTM\", \"Unidirectional LSTM\"],\n",
        "    \"Accuracy\": [bi_acc, uni_acc],\n",
        "    \"F1-score\": [bi_f1, uni_f1],\n",
        "    \"EMBED_DIM\": [EMBED_DIM, EMBED_DIM],\n",
        "    \"LSTM_UNITS\": [LSTM_UNITS, LSTM_UNITS],\n",
        "    \"DROPOUT\": [DROPOUT, DROPOUT],\n",
        "    \"EPOCHS\": [EPOCHS, EPOCHS],\n",
        "    \"BATCH_SIZE\": [BATCH_SIZE, BATCH_SIZE]\n",
        "})\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd3c972",
      "metadata": {
        "id": "0bd3c972"
      },
      "source": [
        "\n",
        "## Observations & Comparison (Write Here)\n",
        "\n",
        "- **Hyperparameters used:**  \n",
        "  - `EMBED_DIM` = *[fill]*, `LSTM_UNITS` = *[fill]*, `DROPOUT` = *[fill]*, `EPOCHS` = *[fill]*, `BATCH_SIZE` = *[fill]*\n",
        "\n",
        "- **Bidirectional vs Unidirectional Results:**  \n",
        "  - **BiLSTM** Accuracy = *[fill]*, F1 = *[fill]*  \n",
        "  - **UniLSTM** Accuracy = *[fill]*, F1 = *[fill]*\n",
        "\n",
        "- **Analysis:**  \n",
        "  - Discuss whether Bidirectional provided better features by reading context **from both past and future** within the sequence compared to Unidirectional, which reads **forward only**.  \n",
        "  - Note if BiLSTM overfits (watch `val_loss`), and how **dropout** or reduced `LSTM_UNITS` affected generalization.  \n",
        "  - Mention any improvements from increasing `EMBED_DIM`, adjusting `MAXLEN`, or changing `VOCAB_SIZE`.\n",
        "\n",
        "- **Further improvements:**  \n",
        "  - Add another LSTM layer (stacked), use **GRU**, try **pretrained embeddings** (e.g., GloVe), or apply **text augmentation** and **regularization** (L2, more dropout).  \n",
        "  - Use **EarlyStopping** and **ModelCheckpoint** callbacks to capture the best validation model.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}