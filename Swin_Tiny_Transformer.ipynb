{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwju5j7NBG2YEn4uRV7wtf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dewwbe/-Real-Estate-Document-Collection-/blob/main/Swin_Tiny_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ü©∫ Chest X-ray Pneumonia Detection using Vision Transformers (Swin-Tiny & ViT-B/16)\n",
        "üìò Project Overview\n",
        "\n",
        "This project focuses on developing a deep learning‚Äìbased diagnostic model to automatically classify chest X-ray images as either NORMAL or PNEUMONIA.\n",
        "We leverage state-of-the-art Transformer architectures ‚Äî specifically Swin-Tiny and ViT-B/16 ‚Äî to explore their capability in medical image analysis and compare their performance.\n",
        "\n",
        "Unlike traditional CNNs, Vision Transformers (ViTs) capture long-range dependencies and contextual cues, which are especially valuable for subtle patterns in medical imaging.\n",
        "The Swin Transformer (Shifted Window Transformer) offers hierarchical, windowed self-attention for better efficiency, while ViT-B/16 uses global attention for strong discriminative features.\n",
        "\n",
        "üéØ Objectives\n",
        "\n",
        "Build and train a binary classifier to detect pneumonia from chest X-rays.\n",
        "\n",
        "Compare Swin-Tiny (trained from scratch or fine-tuned) vs ViT-B/16 (fine-tuned) performance.\n",
        "\n",
        "Analyze training dynamics, evaluation metrics, and calibration reliability.\n",
        "\n",
        "Visualize attention-based interpretability maps to understand model focus regions.\n",
        "\n",
        "‚öôÔ∏è Key Features\n",
        "\n",
        "Data Augmentation: Realistic geometric and photometric transformations to improve robustness.\n",
        "\n",
        "Balanced Sampling: WeightedRandomSampler ensures class balance during training.\n",
        "\n",
        "Mixed Precision Training: Efficient GPU utilization via torch.cuda.amp.\n",
        "\n",
        "Comprehensive Metrics: Accuracy, Precision, Recall, F1, ROC-AUC, Brier score, and calibration plots.\n",
        "\n",
        "Attention Rollout Visualization: Generates interpretable heatmaps highlighting model focus areas.\n",
        "\n",
        "Side-by-Side Comparison: Quantitative and qualitative evaluation of Swin-Tiny vs ViT-B/16.\n",
        "\n",
        "üìä Expected Outcomes\n",
        "\n",
        "High recall (sensitivity) for pneumonia detection ‚Äî prioritizing diagnostic safety.\n",
        "\n",
        "Well-calibrated confidence scores to improve clinical interpretability.\n",
        "\n",
        "Visual explanations showing how transformer attention aligns with pathological regions.\n",
        "\n",
        "üß† Dataset\n",
        "\n",
        "The dataset used follows the standard Chest X-Ray (NORMAL vs PNEUMONIA) structure:\n",
        "\n",
        "/chest_xray/\n",
        "    train/\n",
        "        NORMAL/\n",
        "        PNEUMONIA/\n",
        "    val/\n",
        "        NORMAL/\n",
        "        PNEUMONIA/\n",
        "    test/\n",
        "        NORMAL/\n",
        "        PNEUMONIA/\n",
        "\n",
        "\n",
        "Total samples ‚âà 5,000+ images, with a typical 70/15/15 train‚Äìval‚Äìtest split.\n"
      ],
      "metadata": {
        "id": "i5Q5Eo9Oe5Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß Setup & Library Installation\n",
        "# Installs required packages and imports all dependencies."
      ],
      "metadata": {
        "id": "g3AZg3HvPmjf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQq4-kd6PYl6"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade scikit-learn scipy\n",
        "\n",
        "import os, math, copy, time, random, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms, models, utils as tv_utils\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, confusion_matrix, classification_report,\n",
        "    brier_score_loss\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "print(\"‚úÖ Libraries loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è Configuration and Hyperparameters\n",
        "# Sets up seeds, device, dataset paths, and model/training configs."
      ],
      "metadata": {
        "id": "ty2yQ4PlP6-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/chest_xray\"  # must contain train/, val/, test/\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
        "VAL_DIR   = os.path.join(DATA_DIR, \"val\")\n",
        "TEST_DIR  = os.path.join(DATA_DIR, \"test\")\n",
        "\n",
        "# Model toggles\n",
        "USE_PRETRAINED_SWINT = False   # True = fine-tune ImageNet weights, False = train from scratch\n",
        "RUN_VIT = True                 # True = also run ViT-B/16 comparison\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS_SWINT = 25 if USE_PRETRAINED_SWINT else 80\n",
        "EPOCHS_VIT   = 20\n",
        "PATIENCE = 6\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "IMG_SIZE = 384\n",
        "\n",
        "# Optimization\n",
        "LR_HEAD = 1e-3\n",
        "LR_ALL_SWINT = 3e-5 if USE_PRETRAINED_SWINT else 3e-4\n",
        "LR_ALL_VIT   = 3e-5\n",
        "WEIGHT_DECAY_SWINT = 1e-4 if USE_PRETRAINED_SWINT else 5e-2\n",
        "LABEL_SMOOTH = 0.05 if not USE_PRETRAINED_SWINT else 0.0\n",
        "\n",
        "OUT_DIR = \"/content\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "oTA7e9zpPwE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üñºÔ∏è Data Loading and Exploration"
      ],
      "metadata": {
        "id": "mrrBPS0pP-c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.80, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.02,0.02), scale=(0.98,1.02)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(VAL_DIR,   transform=eval_tfms)\n",
        "test_ds  = datasets.ImageFolder(TEST_DIR,  transform=eval_tfms)\n",
        "class_names = train_ds.classes\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# Balance sampler\n",
        "targets = np.array(train_ds.targets)\n",
        "class_counts = np.bincount(targets)\n",
        "class_weights = 1.0 / (class_counts + 1e-9)\n",
        "sample_weights = class_weights[targets]\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "# Overview\n",
        "def show_dataset_overview(ds, title):\n",
        "    print(f\"{title}: {len(ds)} images | classes={ds.classes}\")\n",
        "    cnt = np.bincount(ds.targets)\n",
        "    for i,c in enumerate(ds.classes):\n",
        "        print(f\"  {c}: {cnt[i]}\")\n",
        "\n",
        "show_dataset_overview(train_ds, \"Train\")\n",
        "show_dataset_overview(val_ds, \"Val\")\n",
        "show_dataset_overview(test_ds, \"Test\")\n",
        "\n",
        "# Sample visualization\n",
        "def show_samples(ds, n=12):\n",
        "    idxs = np.random.choice(len(ds), size=min(n, len(ds)), replace=False)\n",
        "    imgs, labels = zip(*[ds[i] for i in idxs])\n",
        "    grid = tv_utils.make_grid(torch.stack(imgs), nrow=6, padding=2, normalize=True)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.imshow(grid.permute(1,2,0))\n",
        "    plt.axis('off')\n",
        "    plt.title('Random training samples')\n",
        "    plt.show()\n",
        "\n",
        "show_samples(train_ds)"
      ],
      "metadata": {
        "id": "ZLVukDEvPwO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÅ Training Utilities\n",
        "# Contains helper functions for training loops, evaluation, and plotting."
      ],
      "metadata": {
        "id": "HL6wKFPVQKvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "def bce_logits_smooth(logits, targets, eps=0.0):\n",
        "    if eps > 0:\n",
        "        targets = targets*(1-eps) + 0.5*eps\n",
        "    return F.binary_cross_entropy_with_logits(logits, targets)\n",
        "\n",
        "def run_epoch(model, loader, criterion, optimizer=None, grad_clip=None):\n",
        "    train_mode = optimizer is not None\n",
        "    model.train(train_mode)\n",
        "    total_loss = 0.0\n",
        "    y_true, y_prob = [], []\n",
        "    if train_mode:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    for images, labels in loader:\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        labels = labels.float().unsqueeze(1).to(DEVICE, non_blocking=True)\n",
        "        with torch.set_grad_enabled(train_mode):\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "            if train_mode:\n",
        "                scaler.scale(loss).backward()\n",
        "                if grad_clip: torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy().ravel()\n",
        "        y_prob.extend(probs.tolist()); y_true.extend(labels.detach().cpu().numpy().ravel().tolist())\n",
        "    avg_loss = total_loss/len(loader.dataset)\n",
        "    y_true = np.array(y_true); y_prob = np.array(y_prob)\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return avg_loss, acc, y_true, y_prob, y_pred\n",
        "\n",
        "def plot_history(history, title=\"Training Curves\"):\n",
        "    e = range(1, len(history['train_loss'])+1)\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(e, history['train_loss'], label='Train'); plt.plot(e, history['val_loss'], label='Val')\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend(); plt.grid(True, ls='--', alpha=0.4)\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(e, history['train_acc'], label='Train'); plt.plot(e, history['val_acc'], label='Val')\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy'); plt.legend(); plt.grid(True, ls='--', alpha=0.4)\n",
        "    plt.suptitle(title); plt.show()"
      ],
      "metadata": {
        "id": "fEFi1NOgPwV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© Swin-Tiny Model Definition & Training"
      ],
      "metadata": {
        "id": "SbR-M3HkQO1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_swin_tiny(use_pretrained=True):\n",
        "    if use_pretrained:\n",
        "        swin = models.swin_t(weights=models.Swin_T_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        swin = models.swin_t(weights=None)\n",
        "    in_feats = swin.head.in_features\n",
        "    swin.head = nn.Identity()\n",
        "    head = nn.Sequential(\n",
        "        nn.Linear(in_feats, 1024),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm1d(1024),\n",
        "        nn.Dropout(0.3 if use_pretrained else 0.4),\n",
        "        nn.Linear(1024, 256),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.Dropout(0.25 if use_pretrained else 0.3),\n",
        "        nn.Linear(256, 1)\n",
        "    )\n",
        "    model = nn.Sequential(swin, head)\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def fit_swin(model, epochs, ckpt_path, use_pretrained=True):\n",
        "    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n",
        "    # (full training logic same as your version)\n",
        "    # ...\n",
        "    # [Include full fit_swin() function from your code]\n",
        "    return model, history\n",
        "\n",
        "# Train Swin-Tiny\n",
        "swin = build_swin_tiny(USE_PRETRAINED_SWINT)\n",
        "swin_ckpt = os.path.join(OUT_DIR, f\"swin_tiny_{'ft' if USE_PRETRAINED_SWINT else 'scratch'}_best.pth\")\n",
        "swin, swin_hist = fit_swin(swin, EPOCHS_SWINT, swin_ckpt, use_pretrained=USE_PRETRAINED_SWINT)\n",
        "plot_history(swin_hist, f\"Swin-Tiny Training Curves ({'fine-tune' if USE_PRETRAINED_SWINT else 'from scratch'})\")"
      ],
      "metadata": {
        "id": "TY3YG07FPwY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ü§ñ ViT-B/16 Model Fine-tuning #Fine-tunes ViT-B/16 for comparison if RUN_VIT=True."
      ],
      "metadata": {
        "id": "0rPuhePCQWr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if RUN_VIT:\n",
        "    vit = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
        "    vit.heads = nn.Identity()\n",
        "    vit_head = nn.Sequential(\n",
        "        nn.Linear(vit.hidden_dim, 2048), nn.ReLU(inplace=True), nn.BatchNorm1d(2048), nn.Dropout(0.3),\n",
        "        nn.Linear(2048, 512), nn.ReLU(inplace=True), nn.BatchNorm1d(512), nn.Dropout(0.3),\n",
        "        nn.Linear(512, 1)\n",
        "    )\n",
        "    vit = nn.Sequential(vit, vit_head).to(DEVICE)\n",
        "\n",
        "    # Warmup: train head only\n",
        "    for p in vit[0].parameters(): p.requires_grad = False\n",
        "    for p in vit[1].parameters(): p.requires_grad = True\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, vit.parameters()), lr=LR_HEAD, weight_decay=1e-4)\n",
        "    crit = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "\n",
        "    vit_hist = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n",
        "    best_val, best_w, wait = float('inf'), copy.deepcopy(vit.state_dict()), 0\n",
        "    warmup_epochs = 3\n",
        "\n",
        "    for ep in range(1, warmup_epochs+1):\n",
        "        tl, ta, *_ = run_epoch(vit, train_loader, crit, optimizer)\n",
        "        vl, va, *_ = run_epoch(vit, val_loader,   crit, optimizer=None)\n",
        "        vit_hist['train_loss'].append(tl); vit_hist['val_loss'].append(vl)\n",
        "        vit_hist['train_acc'].append(ta);  vit_hist['val_acc'].append(va)\n",
        "        print(f\"[ViT Warmup {ep}/{warmup_epochs}] tl {tl:.4f} ta {ta:.3f} | vl {vl:.4f} va {va:.3f}\")\n",
        "        if vl < best_val: best_val, best_w = vl, copy.deepcopy(vit.state_dict())\n",
        "\n",
        "    # Unfreeze all and fine-tune\n",
        "    for p in vit.parameters(): p.requires_grad = True\n",
        "    optimizer = optim.AdamW(vit.parameters(), lr=LR_ALL_VIT, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_VIT-warmup_epochs, eta_min=1e-6)\n",
        "\n",
        "    for ep in range(warmup_epochs+1, EPOCHS_VIT+1):\n",
        "        tl, ta, *_ = run_epoch(vit, train_loader, crit, optimizer)\n",
        "        vl, va, *_ = run_epoch(vit, val_loader,   crit, optimizer=None)\n",
        "        scheduler.step()\n",
        "        vit_hist['train_loss'].append(tl); vit_hist['val_loss'].append(vl)\n",
        "        vit_hist['train_acc'].append(ta);  vit_hist['val_acc'].append(va)\n",
        "        print(f\"[ViT {ep}/{EPOCHS_VIT}] tl {tl:.4f} ta {ta:.3f} | vl {vl:.4f} va {va:.3f}\")\n",
        "        if vl < best_val: best_val, best_w = vl, copy.deepcopy(vit.state_dict())\n",
        "\n",
        "    vit.load_state_dict(best_w)\n",
        "    vit_ckpt = os.path.join(OUT_DIR, \"vit_b16_ft_best.pth\")\n",
        "    torch.save(vit.state_dict(), vit_ckpt)\n",
        "\n",
        "    plot_history(vit_hist, \"ViT-B/16 Training Curves (fine-tune)\")\n",
        "    print(\"‚úÖ ViT-B/16 fine-tuning completed.\")"
      ],
      "metadata": {
        "id": "b6J61XedYlDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Model Evaluation - Evaluates test performance, ROC, calibration, correlations."
      ],
      "metadata": {
        "id": "4qrzI62pQcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_full(model, loader, title=\"Model\"):\n",
        "    model.eval()\n",
        "    y_true, y_prob = [], []\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.float().unsqueeze(1).to(DEVICE)\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
        "            y_prob.extend(probs.tolist()); y_true.extend(labels.cpu().numpy().ravel().tolist())\n",
        "\n",
        "    y_true = np.array(y_true); y_prob = np.array(y_prob)\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "    tl = total_loss/len(loader.dataset)\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
        "    try: auc = roc_auc_score(y_true, y_prob)\n",
        "    except: auc = float('nan')\n",
        "\n",
        "    print(f\"\\n=== {title} (Test) ===\")\n",
        "    print(f\"Loss: {tl:.4f} | Acc: {acc:.4f} | P: {prec:.4f} | R: {rec:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'ROC (AUC={auc:.3f})')\n",
        "    plt.plot([0,1],[0,1],'--', lw=1)\n",
        "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title(f'ROC - {title}')\n",
        "    plt.grid(True, ls='--', alpha=0.4); plt.legend(); plt.show()\n",
        "\n",
        "    # Calibration\n",
        "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10, strategy='quantile')\n",
        "    bs = brier_score_loss(y_true, y_prob)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(prob_pred, prob_true, marker='o', label='Reliability')\n",
        "    plt.plot([0,1],[0,1],'--', label='Perfect')\n",
        "    plt.xlabel('Predicted probability'); plt.ylabel('Observed frequency')\n",
        "    plt.title(f'Calibration - {title} (Brier={bs:.4f})')\n",
        "    plt.grid(True, ls='--', alpha=0.4); plt.legend(); plt.show()\n",
        "\n",
        "    # Correlations\n",
        "    pr, p_p = pearsonr(y_true, y_prob)\n",
        "    sr, s_p = spearmanr(y_true, y_prob)\n",
        "    print(f\"Pearson r = {pr:.4f} (p={p_p:.3g}) | Spearman œÅ = {sr:.4f} (p={s_p:.3g})\")\n",
        "\n",
        "    return {\n",
        "        'loss': tl, 'accuracy': acc, 'precision': prec, 'recall': rec,\n",
        "        'f1': f1, 'auc': auc, 'brier': bs,\n",
        "        'pearson_r': pr, 'spearman_rho': sr,\n",
        "        'y_true': y_true, 'y_prob': y_prob, 'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "# --- Run evaluations ---\n",
        "swin_test = evaluate_full(swin, test_loader, title=f\"Swin-Tiny ({'FT' if USE_PRETRAINED_SWINT else 'Scratch'})\")\n",
        "if RUN_VIT:\n",
        "    vit_test = evaluate_full(vit, test_loader, title=\"ViT-B/16 (Fine-tune)\")\n"
      ],
      "metadata": {
        "id": "uKWqOPKwQdF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç Interpretability (Attention Rollout)\n",
        "#Exports metrics, training logs, and summary comparisons.\n",
        "#Generates heatmaps similar to Grad-CAM for both Swin and ViT models."
      ],
      "metadata": {
        "id": "fhrCzS-8QhzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_rollout_vitlike(model, images, size=IMG_SIZE, discard_ratio=0.9):\n",
        "    model.eval()\n",
        "    net = model[0] if isinstance(model, nn.Sequential) else model\n",
        "    with torch.no_grad():\n",
        "        x = images.to(DEVICE)\n",
        "        if isinstance(net, models.VisionTransformer):\n",
        "            # ViT path\n",
        "            x_p = net._process_input(x)\n",
        "            x_p = net.conv_proj(x_p).flatten(2).transpose(1,2)\n",
        "            cls = net.class_token.expand(x_p.size(0), -1, -1)\n",
        "            x_p = torch.cat((cls, x_p), dim=1) + net.encoder.pos_embedding\n",
        "            x_p = net.encoder.dropout(x_p)\n",
        "            mats=[]\n",
        "            for blk in net.encoder.layers:\n",
        "                attn_out, attn_weights = blk.attn(x_p, need_weights=True)\n",
        "                mats.append(attn_weights.mean(dim=1))\n",
        "                x_p = blk.ln_1(x_p + attn_out)\n",
        "                x_p = blk.ln_2(x_p + blk.mlp(x_p))\n",
        "            b = mats[0].size(0); N = mats[0].size(-1)\n",
        "            joint = torch.eye(N, device=DEVICE).unsqueeze(0).repeat(b,1,1)\n",
        "            for a in mats:\n",
        "                flat = a.view(b,-1)\n",
        "                k = (flat.size(1)*discard_ratio).round().long()\n",
        "                _, idx = torch.topk(flat, k.item(), dim=1, largest=False)\n",
        "                flat.scatter_(1, idx, 0); a = flat.view_as(a)\n",
        "                a = a/(a.sum(dim=-1, keepdim=True)+1e-6)\n",
        "                joint = a @ joint\n",
        "            mask = joint[:,0,1:]\n",
        "            hw = int(math.sqrt(mask.size(-1))); mask = mask.view(b,1,hw,hw)\n",
        "            mask = F.interpolate(mask, size=(size,size), mode='bilinear', align_corners=False)\n",
        "            mask = (mask - mask.min())/(mask.max()-mask.min()+1e-6)\n",
        "            return mask.cpu()\n",
        "        else:\n",
        "            # Swin path (approximate)\n",
        "            feats = net.features(x)\n",
        "            heat = feats.norm(dim=1, keepdim=True)\n",
        "            heat = F.interpolate(heat, size=(size,size), mode='bilinear', align_corners=False)\n",
        "            heat = (heat - heat.min())/(heat.max()-heat.min()+1e-6)\n",
        "            return heat.cpu()\n",
        "\n",
        "def show_gallery(model, ds, y_true, y_pred, title=\"Interpretability Gallery\"):\n",
        "    idx_tp = np.where((y_true==1) & (y_pred==1))[0]\n",
        "    idx_fp = np.where((y_true==0) & (y_pred==1))[0]\n",
        "    idx_tn = np.where((y_true==0) & (y_pred==0))[0]\n",
        "    idx_fn = np.where((y_true==1) & (y_pred==0))[0]\n",
        "    groups = [(\"TP\", idx_tp), (\"FP\", idx_fp), (\"TN\", idx_tn), (\"FN\", idx_fn)]\n",
        "    for name, idxs in groups:\n",
        "        if len(idxs)==0:\n",
        "            print(f\"No {name} examples.\")\n",
        "            continue\n",
        "        pick = np.random.choice(idxs, size=min(4, len(idxs)), replace=False)\n",
        "        plt.figure(figsize=(10,5))\n",
        "        for j,k in enumerate(pick):\n",
        "            img, _ = ds[k]\n",
        "            vis = (img - img.min())/(img.max()-img.min()+1e-6)\n",
        "            with torch.no_grad():\n",
        "                mask = attention_rollout_vitlike(model, img.unsqueeze(0))\n",
        "            overlay = 0.6*vis + 0.4*mask[0].repeat(3,1,1)\n",
        "            plt.subplot(1,4,j+1); plt.imshow(overlay.permute(1,2,0)); plt.axis('off')\n",
        "        plt.suptitle(f'{title}: {name} (Transformer attention/activation overlays)')\n",
        "        plt.show()\n",
        "\n",
        "show_gallery(swin, test_ds, swin_test['y_true'], swin_test['y_pred'], title='Swin-Tiny')\n",
        "if RUN_VIT:\n",
        "    show_gallery(vit, test_ds, vit_test['y_true'], vit_test['y_pred'], title='ViT-B/16')\n"
      ],
      "metadata": {
        "id": "CZttOkqfPweY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìÅ Save Results & Summary- Exports metrics, training logs, and summary comparisons."
      ],
      "metadata": {
        "id": "uYzzxiL-RDk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üìÅ Save Results & Project Summary\n",
        "# Saves training logs, metrics, checkpoints, and prints summaries.\n",
        "# ============================================================\n",
        "\n",
        "def save_history_csv(history, path_csv): pd.DataFrame(history).to_csv(path_csv, index=False)\n",
        "def save_metrics_csv(metrics_dict, path_csv):\n",
        "    md = {k:v for k,v in metrics_dict.items() if not isinstance(v, np.ndarray)}\n",
        "    pd.DataFrame([md]).to_csv(path_csv, index=False)\n",
        "\n",
        "swin_hist_csv = os.path.join(OUT_DIR, f\"swin_tiny_{'ft' if USE_PRETRAINED_SWINT else 'scratch'}_history.csv\")\n",
        "swin_metrics_csv = os.path.join(OUT_DIR, f\"swin_tiny_{'ft' if USE_PRETRAINED_SWINT else 'scratch'}_test_metrics.csv\")\n",
        "save_history_csv(swin_hist, swin_hist_csv); save_metrics_csv(swin_test, swin_metrics_csv)\n",
        "torch.save(swin.state_dict(), swin_ckpt)\n",
        "\n",
        "if RUN_VIT:\n",
        "    vit_hist_csv = os.path.join(OUT_DIR, \"vit_b16_ft_history.csv\")\n",
        "    vit_metrics_csv = os.path.join(OUT_DIR, \"vit_b16_ft_test_metrics.csv\")\n",
        "    save_history_csv(vit_hist, vit_hist_csv); save_metrics_csv(vit_test, vit_metrics_csv)\n",
        "\n",
        "def print_summary(model_name, history, tm, ds_sizes, img_size, notes):\n",
        "    train_len, val_len, test_len = ds_sizes\n",
        "    print(f\"\\n{model_name.upper()} PNEUMONIA DETECTION - PROJECT SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"DATASET STATISTICS:\")\n",
        "    print(f\"   Training samples: {train_len}\")\n",
        "    print(f\"   Validation samples: {val_len}\")\n",
        "    print(f\"   Test samples: {test_len}\")\n",
        "    print(f\"   Classes: {class_names}\")\n",
        "    print(\"\\nMODEL ARCHITECTURE:\")\n",
        "    total_params = sum(p.numel() for p in tm.parameters())\n",
        "    trainable_params = sum(p.numel() for p in tm.parameters() if p.requires_grad)\n",
        "    print(f\"   Model: {model_name}\")\n",
        "    print(f\"   Input Size: {img_size}x{img_size}x3\")\n",
        "    print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
        "    print(f\"   Total Parameters: {total_params:,}\")\n",
        "    print(f\"   Key Feature: {notes}\")\n",
        "    print(\"\\nTRAINING PERFORMANCE:\")\n",
        "    print(f\"   Final Train Accuracy: {history['train_acc'][-1]:.4f}\")\n",
        "    print(f\"   Final Val   Accuracy: {history['val_acc'][-1]:.4f}\")\n",
        "    print(\"\\nTEST PERFORMANCE:\")\n",
        "    metrics = swin_test if 'Swin' in model_name else tm\n",
        "    print(f\"   Test Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"   Test Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"   Test Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"   Test F1-Score: {metrics['f1']:.4f}\")\n",
        "    print(f\"   Test AUC: {metrics['auc']:.4f}\")\n",
        "    print(f\"   Brier Score: {metrics['brier']:.4f}\")\n",
        "    print(f\"   Pearson r: {metrics['pearson_r']:.4f} | Spearman œÅ: {metrics['spearman_rho']:.4f}\")\n",
        "\n",
        "print_summary(\n",
        "    f\"Swin-Tiny ({'Fine-tune' if USE_PRETRAINED_SWINT else 'From-scratch'})\",\n",
        "    swin_hist, swin, (len(train_ds), len(val_ds), len(test_ds)), IMG_SIZE,\n",
        "    \"Windowed self-attention (data-efficient); transformer overlays for interpretability\"\n",
        ")\n",
        "\n",
        "if RUN_VIT:\n",
        "    print_summary(\n",
        "        \"ViT-B/16 (Fine-tune)\",\n",
        "        vit_hist, vit, (len(train_ds), len(val_ds), len(test_ds)), IMG_SIZE,\n",
        "        \"Global self-attention; transformer overlays for interpretability\"\n",
        "    )\n",
        "\n",
        "if RUN_VIT:\n",
        "    comp = pd.DataFrame([\n",
        "        {\"Model\":\"Swin-Tiny\"+(\" FT\" if USE_PRETRAINED_SWINT else \" Scratch\"),\n",
        "         \"Accuracy\":swin_test['accuracy'], \"Precision\":swin_test['precision'],\n",
        "         \"Recall\":swin_test['recall'], \"F1\":swin_test['f1'], \"AUC\":swin_test['auc'],\n",
        "         \"Brier\":swin_test['brier']},\n",
        "        {\"Model\":\"ViT-B/16 FT\",\n",
        "         \"Accuracy\":vit_test['accuracy'], \"Precision\":vit_test['precision'],\n",
        "         \"Recall\":vit_test['recall'], \"F1\":vit_test['f1'], \"AUC\":vit_test['auc'],\n",
        "         \"Brier\":vit_test['brier']}\n",
        "    ])\n",
        "    print(\"\\nüìä Comparison Table:\\n\", comp)\n"
      ],
      "metadata": {
        "id": "FJ1T3VddPwhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uD8cdZSGPwjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}