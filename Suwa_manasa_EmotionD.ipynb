{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1O3b3U_uKHXy842fVn73ITdl500TWCNm6",
      "authorship_tag": "ABX9TyMKb0No93U8LiwaJTwstNcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dewwbe/-Real-Estate-Document-Collection-/blob/main/Suwa_manasa_EmotionD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 1 — Imports\n",
        "# =========================================\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(\"TF:\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKGKe6zKPMG2",
        "outputId": "70aad849-ae25-4f62-8b9b-f07cfba1870f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 2 — Reproducibility\n",
        "# =========================================\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n"
      ],
      "metadata": {
        "id": "LuX4V8zVPMPe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 3 — Set dataset paths (EDIT ONLY THESE 3)\n",
        "# Adjust these to your environment WITHOUT changing dataset internals.\n",
        "# =========================================\n",
        "\n",
        "# Dataset1 root (contains: \"kidodDataset-Images\" and \"Texts\" folders as you described)\n",
        "DATASET1_ROOT = \"/content/drive/MyDrive/kidoDataset\"   # <- change\n",
        "\n",
        "# Dataset2 root (contains: \"newart/train\" and \"newart/test\")\n",
        "DATASET2_ROOT = \"/content/drive/MyDrive/newart\"   # <- change\n",
        "\n",
        "# Dataset3 voice root (contains: Crema, Ravdess, Savee, Tess like the notebook you pasted)\n",
        "VOICE_ROOT    = \"/content/drive/MyDrive/voicedataset\"  # <- change\n"
      ],
      "metadata": {
        "id": "myUGcxElPMYx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 4 — Helpers (label normalization, safe listing)\n",
        "# =========================================\n",
        "def norm_emotion_label(x: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize label spellings to {happy, sad}.\n",
        "    Handles: Hapiness/Happiness/Happy, Sadness/Sad\n",
        "    \"\"\"\n",
        "    s = x.strip().lower()\n",
        "    s = s.replace(\"hapiness\", \"happy\").replace(\"happiness\", \"happy\")\n",
        "    s = s.replace(\"sadness\", \"sad\")\n",
        "    if s in [\"happy\", \"sad\"]:\n",
        "        return s\n",
        "    return s\n",
        "\n",
        "def safe_listdir(path):\n",
        "    return os.listdir(path) if os.path.exists(path) else []\n",
        "\n",
        "def count_files_recursive(path, exts=None):\n",
        "    exts = exts or []\n",
        "    total = 0\n",
        "    for root, _, files in os.walk(path):\n",
        "        for f in files:\n",
        "            if not exts or any(f.lower().endswith(e) for e in exts):\n",
        "                total += 1\n",
        "    return total\n",
        "\n"
      ],
      "metadata": {
        "id": "G9ZOzytNmKRW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 5 — Build Image datasets from your folder structures\n",
        "# Dataset1:\n",
        "# Images/Emotion/{train,test}/{Hapiness,Sadness}\n",
        "#\n",
        "# Dataset2:\n",
        "# newart/{train,test}/{Happy,Sad,...} -> we ONLY take Happy/Sad for publishable alignment\n",
        "# =========================================\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH = 32\n",
        "\n",
        "# Dataset1 Emotion images\n",
        "D1_IMG_EMO_TRAIN = os.path.join(DATASET1_ROOT, \"Images\", \"Emotion\", \"train\")\n",
        "D1_IMG_EMO_TEST  = os.path.join(DATASET1_ROOT, \"Images\", \"Emotion\", \"test\")\n",
        "\n",
        "# Dataset2 images\n",
        "D2_TRAIN = os.path.join(DATASET2_ROOT, \"train\")\n",
        "D2_TEST  = os.path.join(DATASET2_ROOT, \"test\")\n",
        "\n",
        "print(\"Dataset1 Emotion train exists:\", os.path.exists(D1_IMG_EMO_TRAIN))\n",
        "print(\"Dataset2 train exists:\", os.path.exists(D2_TRAIN))\n",
        "\n",
        "def make_image_ds_from_directory(path, subset=None, validation_split=None):\n",
        "    return tf.keras.utils.image_dataset_from_directory(\n",
        "        path,\n",
        "        labels=\"inferred\",\n",
        "        label_mode=\"int\",\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH,\n",
        "        shuffle=True,\n",
        "        seed=SEED,\n",
        "        validation_split=validation_split,\n",
        "        subset=subset,\n",
        "    )\n",
        "\n",
        "# --- Load Dataset1 Emotion (train/test)\n",
        "ds1_img_train = make_image_ds_from_directory(D1_IMG_EMO_TRAIN)\n",
        "ds1_img_test  = make_image_ds_from_directory(D1_IMG_EMO_TEST)\n",
        "\n",
        "# --- Load Dataset2 BUT only keep Happy/Sad\n",
        "# We'll load full, then filter classes by name mapping.\n",
        "ds2_img_train_full = make_image_ds_from_directory(D2_TRAIN)\n",
        "ds2_img_test_full  = make_image_ds_from_directory(D2_TEST)\n",
        "\n",
        "print(\"D1 classes:\", ds1_img_train.class_names)\n",
        "print(\"D2 classes:\", ds2_img_train_full.class_names)\n",
        "\n",
        "# Build a mapping for Dataset2 indices -> happy/sad or ignore\n",
        "d2_keep = {\"happy\", \"sad\"}\n",
        "d2_idx_to_label = {i: norm_emotion_label(name) for i, name in enumerate(ds2_img_train_full.class_names)}\n",
        "d2_keep_indices = [i for i, lab in d2_idx_to_label.items() if lab in d2_keep]\n",
        "print(\"D2 keep indices:\", d2_keep_indices, \"mapping:\", d2_idx_to_label)\n",
        "\n",
        "def filter_image_ds_to_happy_sad(ds, idx_to_label, keep_indices):\n",
        "    keep_indices_tensor = tf.constant(keep_indices, dtype=tf.int32)\n",
        "\n",
        "    # Unbatch, filter individual elements, then re-batch\n",
        "    ds_unbatched = ds.unbatch()\n",
        "\n",
        "    def element_filter(image, label):\n",
        "        return tf.reduce_any(tf.equal(label, keep_indices_tensor))\n",
        "\n",
        "    ds_filtered_elements = ds_unbatched.filter(element_filter)\n",
        "\n",
        "    # Identify original IDs for happy/sad in this dataset\n",
        "    happy_ids = [i for i in keep_indices_tensor.numpy().tolist() if idx_to_label[i] == \"happy\"]\n",
        "    sad_ids   = [i for i in keep_indices_tensor.numpy().tolist() if idx_to_label[i] == \"sad\"]\n",
        "    if len(happy_ids) != 1 or len(sad_ids) != 1:\n",
        "        raise ValueError(f\"Expected exactly one Happy and one Sad folder. Found happy={happy_ids}, sad={sad_ids}\")\n",
        "\n",
        "    happy_id = tf.constant(happy_ids[0], dtype=tf.int32)\n",
        "    sad_id   = tf.constant(sad_ids[0], dtype=tf.int32)\n",
        "\n",
        "    def element_remap(image, label):\n",
        "        y2 = tf.where(tf.equal(label, happy_id), 0, 1)\n",
        "        return image, y2\n",
        "\n",
        "    ds_remapped_elements = ds_filtered_elements.map(element_remap, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds_remapped_elements.batch(BATCH)\n",
        "\n",
        "ds2_img_train = filter_image_ds_to_happy_sad(ds2_img_train_full, d2_idx_to_label, d2_keep_indices)\n",
        "ds2_img_test  = filter_image_ds_to_happy_sad(ds2_img_test_full,  d2_idx_to_label, d2_keep_indices)\n",
        "\n",
        "# Dataset1 class order can be [Hapiness, Sadness] or similar; remap to {happy:0, sad:1}\n",
        "d1_idx_to_label = {i: norm_emotion_label(name) for i, name in enumerate(ds1_img_train.class_names)}\n",
        "print(\"D1 idx->label:\", d1_idx_to_label)\n",
        "\n",
        "def remap_d1(ds, idx_to_label):\n",
        "    # find ids\n",
        "    happy_ids = [i for i, lab in idx_to_label.items() if lab == \"happy\"]\n",
        "    sad_ids   = [i for i, lab in idx_to_label.items() if lab == \"sad\"]\n",
        "    if len(happy_ids) != 1 or len(sad_ids) != 1:\n",
        "        raise ValueError(f\"Dataset1 Emotion folders must be exactly Hapiness & Sadness. Found {idx_to_label}\")\n",
        "    happy_id = tf.constant(happy_ids[0], dtype=tf.int32)\n",
        "    def _remap(x, y):\n",
        "        y2 = tf.where(tf.equal(y, happy_id), 0, 1)\n",
        "        return x, y2\n",
        "    return ds.map(_remap, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "ds1_img_train = remap_d1(ds1_img_train, d1_idx_to_label)\n",
        "ds1_img_test  = remap_d1(ds1_img_test,  d1_idx_to_label)\n",
        "\n",
        "# Combine dataset1+dataset2 for training/testing\n",
        "img_train = ds1_img_train.concatenate(ds2_img_train).cache().prefetch(tf.data.AUTOTUNE)\n",
        "img_test  = ds1_img_test.concatenate(ds2_img_test).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"✅ Image datasets ready (happy=0, sad=1)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bDI5r21mKaX",
        "outputId": "f098b6d3-d91c-4a9b-fcca-f41313de6a64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset1 Emotion train exists: True\n",
            "Dataset2 train exists: True\n",
            "Found 9228 files belonging to 2 classes.\n",
            "Found 1632 files belonging to 2 classes.\n",
            "Found 694 files belonging to 4 classes.\n",
            "Found 401 files belonging to 4 classes.\n",
            "D1 classes: ['Happiness', 'Sadness']\n",
            "D2 classes: ['Angry', 'Fear', 'Happy', 'Sad']\n",
            "D2 keep indices: [2, 3] mapping: {0: 'angry', 1: 'fear', 2: 'happy', 3: 'sad'}\n",
            "D1 idx->label: {0: 'happy', 1: 'sad'}\n",
            "✅ Image datasets ready (happy=0, sad=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 6 — Load Text dataset from Dataset1 structure\n",
        "# Assuming Emotion_Train.csv and Emotion_Test.csv contain text and label columns.\n",
        "# =========================================\n",
        "\n",
        "D1_TEXT_EMO_TRAIN_FILE = os.path.join(DATASET1_ROOT, \"Texts\", \"Emotion\", \"Emotion_Train.csv\")\n",
        "D1_TEXT_EMO_TEST_FILE  = os.path.join(DATASET1_ROOT, \"Texts\", \"Emotion\", \"Emotion_Test.csv\")\n",
        "\n",
        "print(\"Text train file:\", D1_TEXT_EMO_TRAIN_FILE, \"exists:\", os.path.exists(D1_TEXT_EMO_TRAIN_FILE))\n",
        "print(\"Text test file :\", D1_TEXT_EMO_TEST_FILE,  \"exists:\", os.path.exists(D1_TEXT_EMO_TEST_FILE))\n",
        "\n",
        "# The original infer_label_from_filename and load_text_folder are not suitable\n",
        "# if the data is directly in structured CSVs with 'text' and 'emotion' columns.\n",
        "# We will use pandas to read these files.\n",
        "\n",
        "def load_text_csv(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return [], []\n",
        "    try:\n",
        "        # Read without a header, and then assign column names by index\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "        # Assuming text is in column index 2 and emotion in column index 3\n",
        "        # (0-indexed, so 3rd and 4th columns respectively from your sample output)\n",
        "        text_col_idx = 2\n",
        "        emotion_col_idx = 3\n",
        "\n",
        "        if text_col_idx not in df.columns or emotion_col_idx not in df.columns:\n",
        "            print(f\"Error: Expected columns at index {text_col_idx} (text) and {emotion_col_idx} (emotion) not found in '{file_path}'.\")\n",
        "            print(f\"Available columns indices: {df.columns.tolist()}\")\n",
        "            return [], []\n",
        "\n",
        "        df = df[[text_col_idx, emotion_col_idx]].rename(columns={text_col_idx: 'text', emotion_col_idx: 'emotion'})\n",
        "\n",
        "        # Filter for 'happy' and 'sad' emotions only, normalizing labels\n",
        "        df['emotion'] = df['emotion'].apply(norm_emotion_label)\n",
        "        df_filtered = df[df['emotion'].isin(['happy', 'sad'])]\n",
        "\n",
        "        return df_filtered['text'].tolist(), df_filtered['emotion'].tolist()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        return [], []\n",
        "\n",
        "X_text_train, train_text_labels = load_text_csv(D1_TEXT_EMO_TRAIN_FILE)\n",
        "X_text_test,  test_text_labels  = load_text_csv(D1_TEXT_EMO_TEST_FILE)\n",
        "\n",
        "print(\"Train texts:\", len(X_text_train), \"Test texts:\", len(X_text_test))\n",
        "print(\"Train label counts:\", pd.Series(train_text_labels).value_counts().to_dict())\n",
        "print(\"Test label counts :\", pd.Series(test_text_labels).value_counts().to_dict())\n",
        "\n",
        "y_text_train = np.array([0 if l==\"happy\" else 1 for l in train_text_labels], dtype=np.int32)\n",
        "y_text_test  = np.array([0 if l==\"happy\" else 1 for l in test_text_labels], dtype=np.int32)\n",
        "\n",
        "print(\"✅ Text arrays ready (happy=0, sad=1)\")"
      ],
      "metadata": {
        "id": "RdEyOF5amKdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e65c9f-621f-4513-bfb9-2e4b6bff2173"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text train file: /content/drive/MyDrive/kidoDataset/Texts/Emotion/Emotion_Train.csv exists: True\n",
            "Text test file : /content/drive/MyDrive/kidoDataset/Texts/Emotion/Emotion_Test.csv exists: True\n",
            "Train texts: 9228 Test texts: 1632\n",
            "Train label counts: {'sad': 4614, 'happy': 4614}\n",
            "Test label counts : {'sad': 816, 'happy': 816}\n",
            "✅ Text arrays ready (happy=0, sad=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 7 — Load Voice datasets (Crema, Ravdess, Savee, Tess) and keep only happy/sad\n",
        "# =========================================\n",
        "\n",
        "CREMA_PATH   = os.path.join(VOICE_ROOT, \"Crema\")\n",
        "RAVDESS_PATH = os.path.join(VOICE_ROOT, \"Ravdess\", \"audio_speech_actors_01-24\")\n",
        "SAVEE_PATH   = os.path.join(VOICE_ROOT, \"Savee\")\n",
        "TESS_PATH    = os.path.join(VOICE_ROOT, \"Tess\")\n",
        "\n",
        "print(\"Crema exists:\", os.path.exists(CREMA_PATH))\n",
        "print(\"Ravdess exists:\", os.path.exists(RAVDESS_PATH))\n",
        "print(\"Savee exists:\", os.path.exists(SAVEE_PATH))\n",
        "print(\"Tess exists:\", os.path.exists(TESS_PATH))\n",
        "\n",
        "def load_crema_df(crema_path):\n",
        "    files = safe_listdir(crema_path)\n",
        "    emotions, paths = [], []\n",
        "    for f in files:\n",
        "        if not f.lower().endswith(\".wav\"):\n",
        "            continue\n",
        "        parts = f.split(\"_\")\n",
        "        if len(parts) < 3:\n",
        "            continue\n",
        "        code = parts[2].upper()\n",
        "        if code == \"HAP\":\n",
        "            emotions.append(\"happy\")\n",
        "        elif code == \"SAD\":\n",
        "            emotions.append(\"sad\")\n",
        "        else:\n",
        "            continue\n",
        "        paths.append(os.path.join(crema_path, f))\n",
        "    return pd.DataFrame({\"emotion\": emotions, \"path\": paths})\n",
        "\n",
        "def load_ravdess_df(ravdess_path):\n",
        "    emotions, paths = [], []\n",
        "    actors = safe_listdir(ravdess_path)\n",
        "    for actor_dir in actors:\n",
        "        adir = os.path.join(ravdess_path, actor_dir)\n",
        "        for f in safe_listdir(adir):\n",
        "            if not f.lower().endswith(\".wav\"):\n",
        "                continue\n",
        "            parts = f.split(\".\")[0].split(\"-\")\n",
        "            if len(parts) < 3:\n",
        "                continue\n",
        "            emo_id = int(parts[2])\n",
        "            # 3=happy, 4=sad in RAVDESS\n",
        "            if emo_id == 3:\n",
        "                emotions.append(\"happy\")\n",
        "            elif emo_id == 4:\n",
        "                emotions.append(\"sad\")\n",
        "            else:\n",
        "                continue\n",
        "            paths.append(os.path.join(adir, f))\n",
        "    return pd.DataFrame({\"emotion\": emotions, \"path\": paths})\n",
        "\n",
        "def load_savee_df(savee_path):\n",
        "    files = safe_listdir(savee_path)\n",
        "    emotions, paths = [], []\n",
        "    for f in files:\n",
        "        if not f.lower().endswith(\".wav\"):\n",
        "            continue\n",
        "        # example: 'JE_h09.wav', 'KL_sa13.wav'\n",
        "        token = f.split(\"_\")[1]\n",
        "        token = token[:-6]  # strip digits + '.wav' pattern as in your notebook\n",
        "        if token == \"h\":\n",
        "            emotions.append(\"happy\")\n",
        "        elif token == \"sa\":\n",
        "            emotions.append(\"sad\")\n",
        "        else:\n",
        "            continue\n",
        "        paths.append(os.path.join(savee_path, f))\n",
        "    return pd.DataFrame({\"emotion\": emotions, \"path\": paths})\n",
        "\n",
        "def load_tess_df(tess_path):\n",
        "    emotions, paths = [], []\n",
        "    dirs = safe_listdir(tess_path)\n",
        "    for d in dirs:\n",
        "        subdir = os.path.join(tess_path, d)\n",
        "        for f in safe_listdir(subdir):\n",
        "            if not f.lower().endswith(\".wav\"):\n",
        "                continue\n",
        "            # filename contains emotion token, typical: ..._happy.wav, ..._sad.wav\n",
        "            base = f.split(\".\")[0].lower()\n",
        "            if \"happy\" in base:\n",
        "                emotions.append(\"happy\")\n",
        "            elif re.search(r\"\\bsad\\b\", base) or \"sad\" in base:\n",
        "                emotions.append(\"sad\")\n",
        "            else:\n",
        "                continue\n",
        "            paths.append(os.path.join(subdir, f))\n",
        "    return pd.DataFrame({\"emotion\": emotions, \"path\": paths})\n",
        "\n",
        "crema_df   = load_crema_df(CREMA_PATH)\n",
        "ravdess_df = load_ravdess_df(RAVDESS_PATH)\n",
        "savee_df   = load_savee_df(SAVEE_PATH)\n",
        "tess_df    = load_tess_df(TESS_PATH)\n",
        "\n",
        "voice_df = pd.concat([crema_df, ravdess_df, savee_df, tess_df], ignore_index=True)\n",
        "print(\"Voice samples:\", voice_df.shape)\n",
        "print(voice_df[\"emotion\"].value_counts())\n",
        "\n",
        "# Encode labels: happy=0, sad=1\n",
        "voice_df[\"y\"] = (voice_df[\"emotion\"] == \"sad\").astype(np.int32)\n",
        "\n",
        "# Split\n",
        "voice_train_df, voice_test_df = train_test_split(\n",
        "    voice_df, test_size=0.2, random_state=SEED, stratify=voice_df[\"y\"]\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(voice_train_df), \"Test:\", len(voice_test_df))\n"
      ],
      "metadata": {
        "id": "zr9NxXjlmKf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc9a99e7-e34d-4344-c532-77f2ebd8b9f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crema exists: True\n",
            "Ravdess exists: True\n",
            "Savee exists: True\n",
            "Tess exists: True\n",
            "Voice samples: (3846, 2)\n",
            "emotion\n",
            "happy    1923\n",
            "sad      1923\n",
            "Name: count, dtype: int64\n",
            "Train: 3076 Test: 770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 8 — MFCC extraction + dataset arrays for Audio model\n",
        "# =========================================\n",
        "N_MFCC = 40\n",
        "\n",
        "def extract_mfcc(path, n_mfcc=40, duration=3.0, offset=0.5):\n",
        "    y, sr = librosa.load(path, duration=duration, offset=offset)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "    mfcc = np.mean(mfcc.T, axis=0)  # (n_mfcc,)\n",
        "    return mfcc.astype(np.float32)\n",
        "\n",
        "def build_audio_arrays(df):\n",
        "    X = np.stack([extract_mfcc(p, n_mfcc=N_MFCC) for p in df[\"path\"].values], axis=0)  # (N, 40)\n",
        "    y = df[\"y\"].values.astype(np.int32)\n",
        "    X = np.expand_dims(X, -1)  # (N, 40, 1)\n",
        "    return X, y\n",
        "\n",
        "X_a_train, y_a_train = build_audio_arrays(voice_train_df)\n",
        "X_a_test,  y_a_test  = build_audio_arrays(voice_test_df)\n",
        "\n",
        "print(\"Audio train:\", X_a_train.shape, y_a_train.shape)\n",
        "print(\"Audio test :\", X_a_test.shape,  y_a_test.shape)\n"
      ],
      "metadata": {
        "id": "Z8l2sm1bmKkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2677f91d-0e10-455c-ef57-67072088a54c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio train: (3076, 40, 1) (3076,)\n",
            "Audio test : (770, 40, 1) (770,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 9 — Image model (CNN)\n",
        "# =========================================\n",
        "def build_image_model():\n",
        "    inputs = keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\")(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs, name=\"image_cnn\")\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "img_model = build_image_model()\n",
        "img_model.summary()\n",
        "\n",
        "IMG_EPOCHS = 8\n",
        "hist_img = img_model.fit(img_train, validation_data=img_test, epochs=IMG_EPOCHS)\n"
      ],
      "metadata": {
        "id": "56_RZ2nPmKqA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "e94db73a-e912-4811-f9ab-51411046052e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"image_cnn\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"image_cnn\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,506\u001b[0m (365.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,506</span> (365.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,506\u001b[0m (365.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,506</span> (365.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "    300/Unknown \u001b[1m3062s\u001b[0m 10s/step - accuracy: 0.5481 - loss: 0.6890"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3695s\u001b[0m 12s/step - accuracy: 0.5482 - loss: 0.6890 - val_accuracy: 0.5983 - val_loss: 0.6767\n",
            "Epoch 2/8\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.5924 - loss: 0.6752 - val_accuracy: 0.5999 - val_loss: 0.6660\n",
            "Epoch 3/8\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.5968 - loss: 0.6736 - val_accuracy: 0.5966 - val_loss: 0.6721\n",
            "Epoch 4/8\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.6010 - loss: 0.6719 - val_accuracy: 0.6260 - val_loss: 0.6693\n",
            "Epoch 5/8\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.6086 - loss: 0.6680 - val_accuracy: 0.6315 - val_loss: 0.6528\n",
            "Epoch 6/8\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - accuracy: 0.6112 - loss: 0.6647 - val_accuracy: 0.6358 - val_loss: 0.6575\n",
            "Epoch 7/8\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.6322 - loss: 0.6494 - val_accuracy: 0.6413 - val_loss: 0.6502\n",
            "Epoch 8/8\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.6317 - loss: 0.6496 - val_accuracy: 0.6396 - val_loss: 0.6445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 10 — Text model (TextVectorization + BiLSTM)\n",
        "# =========================================\n",
        "MAX_TOKENS = 20000\n",
        "SEQ_LEN = 200\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LEN\n",
        ")\n",
        "\n",
        "# Ensure all text entries are strings and not None\n",
        "X_text_train = [str(x) for x in X_text_train if x is not None]\n",
        "X_text_test  = [str(x) for x in X_text_test if x is not None]\n",
        "\n",
        "vectorizer.adapt(X_text_train)\n",
        "\n",
        "def build_text_model():\n",
        "    inputs = keras.Input(shape=(), dtype=tf.string)\n",
        "    x = vectorizer(inputs)\n",
        "    x = layers.Embedding(MAX_TOKENS, 128)(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs, name=\"text_bilstm\")\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "text_model = build_text_model()\n",
        "text_model.summary()\n",
        "\n",
        "TEXT_EPOCHS = 6\n",
        "hist_text = text_model.fit(\n",
        "    x=np.array(X_text_train, dtype=object),\n",
        "    y=y_text_train,\n",
        "    validation_data=(np.array(X_text_test, dtype=object), y_text_test),\n",
        "    epochs=TEXT_EPOCHS,\n",
        "    batch_size=BATCH\n",
        ")"
      ],
      "metadata": {
        "id": "8cnL7yJsmYk1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "76cde25d-e69b-45f1-8217-5acfbdd4e398"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"text_bilstm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"text_bilstm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ text_vectorization_3            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ text_vectorization_3            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,659,074\u001b[0m (10.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,659,074</span> (10.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,659,074\u001b[0m (10.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,659,074</span> (10.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - accuracy: 0.7903 - loss: 0.4695 - val_accuracy: 0.9510 - val_loss: 0.1282\n",
            "Epoch 2/6\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.9654 - loss: 0.1022 - val_accuracy: 0.9498 - val_loss: 0.1356\n",
            "Epoch 3/6\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9796 - loss: 0.0663 - val_accuracy: 0.9485 - val_loss: 0.1548\n",
            "Epoch 4/6\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9854 - loss: 0.0468 - val_accuracy: 0.9357 - val_loss: 0.2236\n",
            "Epoch 5/6\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9864 - loss: 0.0410 - val_accuracy: 0.9442 - val_loss: 0.2091\n",
            "Epoch 6/6\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9875 - loss: 0.0411 - val_accuracy: 0.9442 - val_loss: 0.2262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 11 — Audio model (LSTM on MFCC)\n",
        "# =========================================\n",
        "def build_audio_model():\n",
        "    inputs = keras.Input(shape=(N_MFCC, 1))\n",
        "    x = layers.LSTM(128)(inputs)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs, name=\"audio_lstm\")\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "audio_model = build_audio_model()\n",
        "audio_model.summary()\n",
        "\n",
        "AUDIO_EPOCHS = 12\n",
        "hist_audio = audio_model.fit(\n",
        "    X_a_train, y_a_train,\n",
        "    validation_data=(X_a_test, y_a_test),\n",
        "    epochs=AUDIO_EPOCHS,\n",
        "    batch_size=64\n",
        ")\n"
      ],
      "metadata": {
        "id": "1d0jbhOHmYb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "outputId": "b0b44726-1610-40b5-d890-d099f965102f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"audio_lstm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"audio_lstm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m66,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m74,946\u001b[0m (292.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,946</span> (292.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m74,946\u001b[0m (292.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,946</span> (292.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5399 - loss: 0.6798 - val_accuracy: 0.6727 - val_loss: 0.5882\n",
            "Epoch 2/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6722 - loss: 0.5886 - val_accuracy: 0.7260 - val_loss: 0.5373\n",
            "Epoch 3/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7929 - loss: 0.4508 - val_accuracy: 0.7831 - val_loss: 0.4592\n",
            "Epoch 4/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8233 - loss: 0.4106 - val_accuracy: 0.8039 - val_loss: 0.4006\n",
            "Epoch 5/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8170 - loss: 0.4021 - val_accuracy: 0.8156 - val_loss: 0.3953\n",
            "Epoch 6/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8212 - loss: 0.3834 - val_accuracy: 0.8234 - val_loss: 0.3752\n",
            "Epoch 7/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8238 - loss: 0.3648 - val_accuracy: 0.8195 - val_loss: 0.3729\n",
            "Epoch 8/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8343 - loss: 0.3601 - val_accuracy: 0.8260 - val_loss: 0.3537\n",
            "Epoch 9/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8283 - loss: 0.3556 - val_accuracy: 0.8351 - val_loss: 0.3299\n",
            "Epoch 10/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8363 - loss: 0.3440 - val_accuracy: 0.8494 - val_loss: 0.3292\n",
            "Epoch 11/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8518 - loss: 0.3271 - val_accuracy: 0.8377 - val_loss: 0.3231\n",
            "Epoch 12/12\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8502 - loss: 0.3369 - val_accuracy: 0.8351 - val_loss: 0.3240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 12 — Late Fusion predictor\n",
        "# =========================================\n",
        "def predict_fusion(image_batch, text_batch, audio_batch, w=(1.0, 1.0, 1.0)):\n",
        "    \"\"\"\n",
        "    image_batch: (B,224,224,3)\n",
        "    text_batch : list/array of strings length B\n",
        "    audio_batch: (B,40,1)\n",
        "    w = weights (wi, wt, wa)\n",
        "    \"\"\"\n",
        "    wi, wt, wa = w\n",
        "    p_img  = img_model.predict(image_batch, verbose=0)\n",
        "    p_text = text_model.predict(np.array(text_batch, dtype=object), verbose=0)\n",
        "    p_aud  = audio_model.predict(audio_batch, verbose=0)\n",
        "\n",
        "    p = (wi*p_img + wt*p_text + wa*p_aud) / (wi + wt + wa)\n",
        "    return p  # (B,2)\n"
      ],
      "metadata": {
        "id": "o6T78qhTmlOW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 13 — Quick sanity test for fusion with 1 batch each (if available)\n",
        "# This is just a demonstration of the fusion pipeline.\n",
        "# =========================================\n",
        "\n",
        "# Get one image batch from img_test\n",
        "for xb_img, yb_img in img_test.take(1):\n",
        "    sample_img = xb_img.numpy()\n",
        "    break\n",
        "\n",
        "# Get one text batch from text test\n",
        "B = min(16, len(X_text_test))\n",
        "sample_text = X_text_test[:B]\n",
        "\n",
        "# Get one audio batch from audio test\n",
        "sample_audio = X_a_test[:B]\n",
        "\n",
        "# Make sizes consistent (same B)\n",
        "B2 = min(sample_img.shape[0], len(sample_text), sample_audio.shape[0])\n",
        "p_fused = predict_fusion(sample_img[:B2], sample_text[:B2], sample_audio[:B2])\n",
        "\n",
        "print(\"Fusion probs shape:\", p_fused.shape)\n",
        "print(\"First 5 probs [happy,sad]:\\n\", p_fused[:5])\n",
        "print(\"Pred labels (0=happy,1=sad):\", np.argmax(p_fused, axis=1)[:20])\n"
      ],
      "metadata": {
        "id": "lTXkRRbjmnl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4e5f72-5368-4394-8914-5116f9195093"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fusion probs shape: (16, 2)\n",
            "First 5 probs [happy,sad]:\n",
            " [[0.4746636  0.52533644]\n",
            " [0.5741978  0.42580214]\n",
            " [0.3136351  0.6863649 ]\n",
            " [0.29673654 0.70326346]\n",
            " [0.42825612 0.57174385]]\n",
            "Pred labels (0=happy,1=sad): [1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 14 — Save\n",
        "# =========================================\n",
        "img_model.save(\"image_emotion_happy_sad.keras\")\n",
        "text_model.save(\"text_emotion_happy_sad.keras\")\n",
        "audio_model.save(\"audio_emotion_happy_sad.keras\")\n",
        "print(\"✅ Saved all 3 models.\")\n"
      ],
      "metadata": {
        "id": "m_TQJNenmrpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eda99c1-dcc7-4678-db3e-8392ee8762b7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved all 3 models.\n"
          ]
        }
      ]
    }
  ]
}