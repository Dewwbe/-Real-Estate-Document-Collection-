{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dewwbe/-Real-Estate-Document-Collection-/blob/main/pandas_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3XE-GC7PvOBN",
      "metadata": {
        "id": "3XE-GC7PvOBN"
      },
      "source": [
        "# Pandas Complete tutorial for data science in 2022\n",
        "\n",
        "This is part of the original article [Pandas Complete tutorial for data science in 2022](https://norochalise.medium.com/pandas-complete-tutorial-for-data-science-in-2022-685a4c6df347)\n",
        "\n",
        "https://github.com/norochalise/pandas-tutorial-article-2022"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q72dXwaog4h7",
      "metadata": {
        "id": "q72dXwaog4h7"
      },
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vSvx8LjDp6Yh",
      "metadata": {
        "id": "vSvx8LjDp6Yh"
      },
      "source": [
        "### Import\n",
        "\n",
        "Before moving on to learn pandas first we need to install it and import it. If you install [Anaconda distributions](https://www.anaconda.com/) on your local machine or using [Google Colab](https://research.google.com/colaboratory) then pandas will already be available there, otherwise, you follow this installation process from [pandas official's website](https://pandas.pydata.org/docs/getting_started/install.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e7dcee3a",
      "metadata": {
        "id": "e7dcee3a"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "CeSsGkTMiMcI",
      "metadata": {
        "id": "CeSsGkTMiMcI"
      },
      "outputs": [],
      "source": [
        "# we can set numbers for how many rows and columns will be displayed\n",
        "pd.set_option('display.min_rows', 10) #default will be 10\n",
        "pd.set_option('display.max_columns', 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68cb06b1",
      "metadata": {
        "id": "68cb06b1"
      },
      "source": [
        "## 2. Loading Different Data Formats Into a Pandas Data Frame\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7_X0klbRI24J",
      "metadata": {
        "id": "7_X0klbRI24J"
      },
      "source": [
        "### Reading CSV file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "56739039",
      "metadata": {
        "id": "56739039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "ed029167-4930-4c8b-e858-6de29ec5d4fb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'online_store_customer_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4042383454.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'online_store_customer_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'online_store_customer_data.csv'"
          ]
        }
      ],
      "source": [
        "# read csv file\n",
        "\n",
        "df = pd.read_csv('online_store_customer_data.csv')\n",
        "\n",
        "df.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(5)"
      ],
      "metadata": {
        "id": "FVjKUDT32bSJ"
      },
      "id": "FVjKUDT32bSJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jNRVw6Xk_zGE",
      "metadata": {
        "id": "jNRVw6Xk_zGE"
      },
      "outputs": [],
      "source": [
        "# Loading csv file with skip first 2 rows without header\n",
        "df_csv = pd.read_csv('online_store_customer_data.csv', skiprows=2, header=None)\n",
        "df_csv.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qNhba27PwpUa",
      "metadata": {
        "id": "qNhba27PwpUa"
      },
      "source": [
        "### Read CSV file from URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZdhOBOE7xXW0",
      "metadata": {
        "id": "ZdhOBOE7xXW0"
      },
      "outputs": [],
      "source": [
        "# Read csv file from url\n",
        "url=\"https://raw.githubusercontent.com/norochalise/medium/main/pandas_tutorial/dataset/online_store_customer_data.csv\"\n",
        "df_url = pd.read_csv(url)\n",
        "df_url.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h6bQJPx5zJEm",
      "metadata": {
        "id": "h6bQJPx5zJEm"
      },
      "source": [
        "### Write CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uykhkL_7zSMu",
      "metadata": {
        "id": "uykhkL_7zSMu"
      },
      "outputs": [],
      "source": [
        "# saving df_url dataframe to csv file\n",
        "df1 = pd.read_csv('online_store_customer_data.csv', skiprows=2, header=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hZCC2_gp1Qcm",
      "metadata": {
        "id": "hZCC2_gp1Qcm"
      },
      "source": [
        "### Read text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i9Adg1q32Hkj",
      "metadata": {
        "id": "i9Adg1q32Hkj"
      },
      "outputs": [],
      "source": [
        "# read plain text file\n",
        "df_txt = pd.read_csv(\"dataset/demo_text.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W0hKEl_pJ4M3",
      "metadata": {
        "id": "W0hKEl_pJ4M3"
      },
      "source": [
        "### Read Excel file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qb_pWVKFMq4E",
      "metadata": {
        "id": "Qb_pWVKFMq4E"
      },
      "outputs": [],
      "source": [
        "# read excel file\n",
        "df_excel = pd.read_excel('dataset/excel_file.xlsx', sheet_name='Sheet1')\n",
        "df_excel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4JZevqX3N0Zl",
      "metadata": {
        "id": "4JZevqX3N0Zl"
      },
      "source": [
        "### Write Excel file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x8soGD0I02RB",
      "metadata": {
        "id": "x8soGD0I02RB"
      },
      "outputs": [],
      "source": [
        "# save dataframe to the excel file\n",
        "df_url.to_csv('demo.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10a5b306",
      "metadata": {
        "id": "10a5b306"
      },
      "source": [
        "## 3. Data preprocessing\n",
        "Data preprocessing is the process of making raw data to clean data. This is the most crucial part of data the science. In this section, we will explore data first then we remove unwanted columns, remove duplicates, handle missing data, etc. After this step, we get clean data from raw data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2098e35f",
      "metadata": {
        "id": "2098e35f"
      },
      "source": [
        "### 3.1 Data Exploring"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aHKv7GbtBGG5",
      "metadata": {
        "id": "aHKv7GbtBGG5"
      },
      "source": [
        "#### Retrieving rows from data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc7a3df",
      "metadata": {
        "id": "cbc7a3df"
      },
      "outputs": [],
      "source": [
        "# display first 3 rows\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d4f7c8",
      "metadata": {
        "id": "58d4f7c8"
      },
      "outputs": [],
      "source": [
        "# display last 6 rows\n",
        "df.tail(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1L2Pg_LiChrC",
      "metadata": {
        "id": "1L2Pg_LiChrC"
      },
      "source": [
        "#### Retrieving sample rows from data frame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e099c7",
      "metadata": {
        "id": "36e099c7"
      },
      "outputs": [],
      "source": [
        "# Display random 7 sample rows\n",
        "df.sample(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A5NQiaXLlmS3",
      "metadata": {
        "id": "A5NQiaXLlmS3"
      },
      "source": [
        "#### Retrieving information about dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74baf9db",
      "metadata": {
        "id": "74baf9db"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "Ml2XunC5SEan"
      },
      "id": "Ml2XunC5SEan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40639f0a",
      "metadata": {
        "id": "40639f0a"
      },
      "outputs": [],
      "source": [
        "# display datatypes\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad7ad16",
      "metadata": {
        "id": "5ad7ad16"
      },
      "outputs": [],
      "source": [
        "df.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XfPcQQb2IH-u",
      "metadata": {
        "id": "XfPcQQb2IH-u"
      },
      "source": [
        "#### Display number of rows and columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740fc7ae",
      "metadata": {
        "id": "740fc7ae"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdX_PamDE2Ko",
      "metadata": {
        "id": "fdX_PamDE2Ko"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5w-MfQeS2m6"
      },
      "id": "h5w-MfQeS2m6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T3t93wXoFTeL",
      "metadata": {
        "id": "T3t93wXoFTeL"
      },
      "outputs": [],
      "source": [
        "# display Age columns first 3 rows data\n",
        "df1 = df[['Age','State_names']]\n",
        "df1.to_csv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x7TKiUXBF4RE",
      "metadata": {
        "id": "x7TKiUXBF4RE"
      },
      "outputs": [],
      "source": [
        "# display first 4 rows of Age, Transaction_date and Gender columns\n",
        "df[['Age', 'Marital_status', 'Gender']].sample(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bkh1oFtuoSiS",
      "metadata": {
        "id": "Bkh1oFtuoSiS"
      },
      "source": [
        "#### Retrieving a Range of Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20qnnoofpPfn",
      "metadata": {
        "id": "20qnnoofpPfn"
      },
      "outputs": [],
      "source": [
        "# for display 2nd to 6th rows\n",
        "df[2:7]\n",
        "# for display starting to 10th\n",
        "df[11:]\n",
        "\n",
        "# for display last two rows\n",
        "df[-2:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[-5:]"
      ],
      "metadata": {
        "id": "a-l-b46XUGmO"
      },
      "id": "a-l-b46XUGmO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.loc[df.Gender == \"Male\",\"Gender\"] = 0"
      ],
      "metadata": {
        "id": "Upz-39J9Upp4"
      },
      "id": "Upz-39J9Upp4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Ak6Ls2lmVzB0"
      },
      "id": "Ak6Ls2lmVzB0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "03dec553",
      "metadata": {
        "id": "03dec553"
      },
      "source": [
        " ### 3.2 Data Cleaning\n",
        "After the explore our datasets may need to clean them for better analysis. Data coming in from multiple sources so It's possible to have an error in some values. This is where data cleaning becomes extremely important. In this section, we will delete unwanted columns, rename columns, correct appropriate data types, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HGxEahSYJ0Ft",
      "metadata": {
        "id": "HGxEahSYJ0Ft"
      },
      "source": [
        "#### Delete Columns name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "43b62a5f",
      "metadata": {
        "id": "43b62a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "58fd9775-d4c7-402d-d419-f455e3eb5741"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-446240328.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Drop unwanted columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Amount_spent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Drop unwanted columns\n",
        "df.drop(['Amount_spent'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "xHbz__Lj7S00"
      },
      "id": "xHbz__Lj7S00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "jtlvWfg7KR_Q",
      "metadata": {
        "id": "jtlvWfg7KR_Q"
      },
      "source": [
        "#### Change Columns name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0StjAOVBKdMR",
      "metadata": {
        "id": "0StjAOVBKdMR"
      },
      "outputs": [],
      "source": [
        "# create new df_col dataframe from df.copy() method.\n",
        "df_col = df.copy()\n",
        "\n",
        "# rename columns name\n",
        "df.rename(columns={\"Transaction_date\": \"Date\", \"Gender\": \"Sex\"}, inplace=True)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HXspyxQwg5wn",
      "metadata": {
        "id": "HXspyxQwg5wn"
      },
      "source": [
        "#### Adding a new column to a DataFrame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "te1hhuRlhN-l",
      "metadata": {
        "id": "te1hhuRlhN-l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "2de5f230-7e97-4543-abec-55edeba13a34"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_col' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-920185770.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add a new ajusted column which value will be amount_spent * 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_col\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_col'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_col\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf_col\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_col' is not defined"
          ]
        }
      ],
      "source": [
        "# Add a new ajusted column which value will be amount_spent * 100\n",
        "df_col['new_col'] = df_col['Age'] + df_col['Age']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bXmRJsHig4t",
      "metadata": {
        "id": "2bXmRJsHig4t"
      },
      "outputs": [],
      "source": [
        "df_col.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wEGhdvVnKiDy",
      "metadata": {
        "id": "wEGhdvVnKiDy"
      },
      "source": [
        "#### String value change or replace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "juaJjLGrFvP_",
      "metadata": {
        "id": "juaJjLGrFvP_"
      },
      "outputs": [],
      "source": [
        "df_col.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qYOI7nb2KpfL",
      "metadata": {
        "id": "qYOI7nb2KpfL"
      },
      "outputs": [],
      "source": [
        "# changing Female to Woman and Male to Man in Sex column.\n",
        "#first argument in loc function is condition and second one is columns name.\n",
        "df_col.loc[df_col.Sex == \"Female\", 'Sex'] = 'Woman'\n",
        "df_col.loc[df_col.Sex == \"Male\", 'Sex'] = 'Man'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_col.State_names == \"Kansas\""
      ],
      "metadata": {
        "id": "8m-7dqBt93bO"
      },
      "id": "8m-7dqBt93bO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_col.loc[df_col.State_names == \"Kansas\"]"
      ],
      "metadata": {
        "id": "bv0omr4d9kqO"
      },
      "id": "bv0omr4d9kqO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q7EoRcMSF132",
      "metadata": {
        "id": "Q7EoRcMSF132"
      },
      "outputs": [],
      "source": [
        "df_col.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3JWg0IavF531",
      "metadata": {
        "id": "3JWg0IavF531"
      },
      "source": [
        "Now Sex columns values are changed Female to Woman and Male to Man."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T7sc_D_N267m",
      "metadata": {
        "id": "T7sc_D_N267m"
      },
      "source": [
        "#### Datatypes change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uINSYaj54XnX",
      "metadata": {
        "id": "uINSYaj54XnX"
      },
      "outputs": [],
      "source": [
        "df_col.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IHa8ZUo84cVD",
      "metadata": {
        "id": "IHa8ZUo84cVD"
      },
      "source": [
        "In our `Date` columns, it's object type so now we will convert this to date types, and also we will convert `Referal` columns float64 to float32.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8yfIDgFg4zhY",
      "metadata": {
        "id": "8yfIDgFg4zhY"
      },
      "outputs": [],
      "source": [
        "# change object type to datefime64 format\n",
        "df_col['Date'] = df_col['Date'].astype('datetime64[ns]')\n",
        "\n",
        "# change float64 to float32 of Referal columns\n",
        "df_col['Referal'] = df_col['Referal'].astype('float32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V6Ux9BVcBJZc",
      "metadata": {
        "id": "V6Ux9BVcBJZc"
      },
      "outputs": [],
      "source": [
        "df_col.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_col.describe()"
      ],
      "metadata": {
        "id": "4-6wdXi6D7d2"
      },
      "id": "4-6wdXi6D7d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dcf97600",
      "metadata": {
        "id": "dcf97600"
      },
      "source": [
        "### 3.3 Remove duplicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc549528",
      "metadata": {
        "id": "dc549528"
      },
      "outputs": [],
      "source": [
        "# Display duplicated entries\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"State_names\"].value_counts()"
      ],
      "metadata": {
        "id": "fJYIdtKu_KDn"
      },
      "id": "fJYIdtKu_KDn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00fe073d",
      "metadata": {
        "id": "00fe073d"
      },
      "outputs": [],
      "source": [
        "# duplicate rows dispaly, keep arguments will--- 'first', 'last' and False\n",
        "duplicate_value = df.duplicated(keep='first')\n",
        "\n",
        "df.loc[duplicate_value, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb4d3eb",
      "metadata": {
        "id": "6fb4d3eb"
      },
      "outputs": [],
      "source": [
        "# dropping ALL duplicate values\n",
        "df.drop_duplicates(keep = 'first', inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1e3272",
      "metadata": {
        "id": "de1e3272"
      },
      "source": [
        "### 3.4 Handling missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9y1tYxbzLWw1",
      "metadata": {
        "id": "9y1tYxbzLWw1"
      },
      "source": [
        "Handling missing values in the common task in the data pre-processing part. For many reasons most of the time we will encounter missing values. Without dealing with this we can't do the proper model building. For this section first, we will find out missing values then we decided how to handle them. We can handle this by removing affected columns or rows or replacing appropriate values there."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44ff83e8",
      "metadata": {
        "id": "44ff83e8"
      },
      "source": [
        "#### Display missing values information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ecbaa3",
      "metadata": {
        "id": "e6ecbaa3"
      },
      "outputs": [],
      "source": [
        "df.isna().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5UgQzwrITZk3",
      "metadata": {
        "id": "5UgQzwrITZk3"
      },
      "source": [
        "#### Delete Nan rows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5CV_qntnTpja",
      "metadata": {
        "id": "5CV_qntnTpja"
      },
      "source": [
        "If we have less Nan value then we can delete entire rows by `dropna()` function. For this function, we will add columns name in subset parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbb429f",
      "metadata": {
        "id": "3bbb429f"
      },
      "outputs": [],
      "source": [
        "# df copy to df_copy\n",
        "df_new = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fe65d5",
      "metadata": {
        "id": "99fe65d5"
      },
      "outputs": [],
      "source": [
        "#Delete Nan rows of Job Columns\n",
        "df_new.dropna(subset = [\"Employees_status\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bxAq9hnUK4R",
      "metadata": {
        "id": "6bxAq9hnUK4R"
      },
      "source": [
        "#### Delete entire columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UIsVP0RAUS75",
      "metadata": {
        "id": "UIsVP0RAUS75"
      },
      "source": [
        "If we have a large number of nan values in particular columns then dropping those columns might be a good decision rather than imputing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4400782c",
      "metadata": {
        "id": "4400782c"
      },
      "outputs": [],
      "source": [
        "df_new.drop(columns=['Amount_spent'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29243a31",
      "metadata": {
        "id": "29243a31"
      },
      "outputs": [],
      "source": [
        "df_new.isna().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14-WG_ynUomI",
      "metadata": {
        "id": "14-WG_ynUomI"
      },
      "source": [
        "#### Impute missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jECrC9RYVJX6",
      "metadata": {
        "id": "jECrC9RYVJX6"
      },
      "source": [
        "Sometimes if we delete entire columns that will be not the appropriate approach. Delete columns can affect our model building because we will lose our main features. For imputing we have many approaches so here are some of the most popular techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a4435f",
      "metadata": {
        "id": "c1a4435f"
      },
      "source": [
        "**Method 1** - Impute fixed value like 0, 'Unknown' or 'Missing' etc. We inpute Unknown in Gender columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb6c602",
      "metadata": {
        "id": "dfb6c602"
      },
      "outputs": [],
      "source": [
        "df['Gender'].fillna('Unknown', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615ac388",
      "metadata": {
        "id": "615ac388"
      },
      "source": [
        "**Method 2** - Impute Mean, Median and Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a25d8d4",
      "metadata": {
        "id": "5a25d8d4"
      },
      "outputs": [],
      "source": [
        "# Impute Mean in Amount_spent columns\n",
        "mean_amount_spent = df['Amount_spent'].mean()\n",
        "df['Amount_spent'].fillna(mean_amount_spent, inplace=True)\n",
        "\n",
        "#Impute Median in Age column\n",
        "median_age = df['Age'].median()\n",
        "df['Age'].fillna(median_age, inplace=True)\n",
        "\n",
        "# Impute Mode in Employees_status column\n",
        "mode_emp = df['Employees_status'].mode().iloc[0]\n",
        "df['Employees_status'].fillna(mode_emp, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ed0805",
      "metadata": {
        "id": "13ed0805"
      },
      "source": [
        "**Method 3** - Imputing forward fill or backfill by `ffill` and `bfill`. In `ffill` missing value impute from the value of the above row and for `bfill` it's taken from the below rows value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f251fd",
      "metadata": {
        "id": "f9f251fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['Referal'].fillna(method='ffill', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OEFYI75II6PN",
      "metadata": {
        "id": "OEFYI75II6PN"
      },
      "outputs": [],
      "source": [
        "df.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AuU7koGzcbPy",
      "metadata": {
        "id": "AuU7koGzcbPy"
      },
      "source": [
        "Now we deal with all missing values with different methods. So now we haven't any null values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2980dcd0",
      "metadata": {
        "id": "2980dcd0"
      },
      "source": [
        "## 4. Memory management"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4hM8Z6ptQwAM",
      "metadata": {
        "id": "4hM8Z6ptQwAM"
      },
      "source": [
        "When we work on large datasets, There we get one big issue is a memory problem. We need too large resources for dealing with this. But there are some methods in pandas to deal with this. Here are some methods or strategies to deal with this problem with help of pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QyO24xKLTEEm",
      "metadata": {
        "id": "QyO24xKLTEEm"
      },
      "source": [
        "### Change datatypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a40362",
      "metadata": {
        "id": "a8a40362"
      },
      "outputs": [],
      "source": [
        "df_memory = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5587bc4",
      "metadata": {
        "id": "c5587bc4"
      },
      "outputs": [],
      "source": [
        "memory_usage = df_memory.memory_usage(deep=True)\n",
        "memory_usage_in_mbs = round(np.sum(memory_usage / 1024 ** 2), 3)\n",
        "print(f\" Total memory taking df_memory dataframe is : {memory_usage_in_mbs:.2f} MB \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YC_ucd2fXzjD",
      "metadata": {
        "id": "YC_ucd2fXzjD"
      },
      "source": [
        "#### Change object to category datatypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cclw14OrWEia",
      "metadata": {
        "id": "Cclw14OrWEia"
      },
      "source": [
        "Our data frame is small in size. Which is 1.15 MB. Now We will convert our object datatype to category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1dfe2d1",
      "metadata": {
        "id": "c1dfe2d1"
      },
      "outputs": [],
      "source": [
        "# Object datatype to category convert\n",
        "df_memory[df_memory.select_dtypes(['object']).columns] = df_memory.select_dtypes(['object']).apply(lambda x: x.astype('category'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a5daead",
      "metadata": {
        "id": "3a5daead"
      },
      "outputs": [],
      "source": [
        "# convert object to category\n",
        "df_memory.info(memory_usage=\"deep\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NcPyDr0WlDJ",
      "metadata": {
        "id": "9NcPyDr0WlDJ"
      },
      "source": [
        "Now its reduce 1.15 megabytes to 216.6 kb. It's almost reduced 5.5 times."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OAhUpeaVYBDX",
      "metadata": {
        "id": "OAhUpeaVYBDX"
      },
      "source": [
        "#### Change int64 or float64 to int 32, 16, or 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JRp-9sgzbK-y",
      "metadata": {
        "id": "JRp-9sgzbK-y"
      },
      "source": [
        "By default, pandas store numeric values to int64 or float64. Which takes more memory. If we have to store small numbers then we can change to 64 to 32, 16, and so on. For example, our Referal columns have only 0 and 1 values so for that we don't need to store at float64. so now we change it to float16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4208c6e",
      "metadata": {
        "id": "a4208c6e"
      },
      "outputs": [],
      "source": [
        "# Change Referal column datatypes\n",
        "df_memory['Referal'] = df_memory['Referal'].astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680accae",
      "metadata": {
        "id": "680accae"
      },
      "outputs": [],
      "source": [
        "# convert object to category\n",
        "df_memory.info(memory_usage=\"deep\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tDkU2zSZdBfA",
      "metadata": {
        "id": "tDkU2zSZdBfA"
      },
      "source": [
        "After changing only one column's data types we reduce 216 kb to 179 kb.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XtsqLqzyeJ2q",
      "metadata": {
        "id": "XtsqLqzyeJ2q"
      },
      "source": [
        "**Note: Before changing datatypes please make sure it's consequences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d6f71f",
      "metadata": {
        "id": "d9d6f71f"
      },
      "source": [
        "## 5. Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e3b208b",
      "metadata": {
        "id": "4e3b208b"
      },
      "source": [
        "### 5.1. Calculating Basic statistical measurement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f30a11",
      "metadata": {
        "id": "36f30a11"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n6s8tjhWLkxS",
      "metadata": {
        "id": "n6s8tjhWLkxS"
      },
      "source": [
        "We know already above code will display only numeric columns basic statistical information. for object or category columns we can use `describe(include=object)` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bedd8b71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "bedd8b71",
        "outputId": "cd5dc388-1bc5-4e94-dbab-5f4ba0cae1c2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2435359588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "df.describe(include=object).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f_p8W2lmL-C7",
      "metadata": {
        "id": "f_p8W2lmL-C7"
      },
      "source": [
        "We can calculate the mean, median, mode, maximum values, minimum values of individual columns we simply use these functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60ff776",
      "metadata": {
        "id": "c60ff776"
      },
      "outputs": [],
      "source": [
        "# Calculate Mean\n",
        "mean = df['Age'].mean()\n",
        "\n",
        "# Calculate Median\n",
        "median = df['Age'].median()\n",
        "\n",
        "#Calculate Mode\n",
        "mode = df['Age'].mode().iloc[0]\n",
        "\n",
        "# Calculate standard deviation\n",
        "std = df['Age'].std()\n",
        "\n",
        "# Calculate Minimum values\n",
        "minimum = df['Age'].min()\n",
        "\n",
        "# Calculate Maximum values\n",
        "maximum = df.Age.max()\n",
        "\n",
        "print(f\" Mean of Age : {mean}\")\n",
        "print(f\" Median of Age : {median}\")\n",
        "print(f\" Mode of Age : {mode}\")\n",
        "print(f\" Standard deviation of Age : {std:.2f}\")\n",
        "print(f\" Maximum of Age : {maximum}\")\n",
        "print(f\" Menimum of Age : {minimum}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dd---Bu-Ms-R",
      "metadata": {
        "id": "Dd---Bu-Ms-R"
      },
      "source": [
        "In pandas we can display the correlation of different numeric columns. For this we can use `.corr()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e89ace8",
      "metadata": {
        "id": "8e89ace8"
      },
      "outputs": [],
      "source": [
        "# calculate correlation\n",
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CZnzwDXQg35f",
      "metadata": {
        "id": "CZnzwDXQg35f"
      },
      "source": [
        "### 5.2 Basic built in function for data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dUAN1hwfe8m",
      "metadata": {
        "id": "1dUAN1hwfe8m"
      },
      "source": [
        "#### Number of uniqe values in category column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b77672",
      "metadata": {
        "id": "01b77672"
      },
      "outputs": [],
      "source": [
        "# for display how many unique values are there in State_names column\n",
        "df['State_names'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uKCSrDSDI7Bs",
      "metadata": {
        "id": "uKCSrDSDI7Bs"
      },
      "source": [
        "#### Shows all unique values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca391db",
      "metadata": {
        "id": "5ca391db"
      },
      "outputs": [],
      "source": [
        "# for display uniqe values of State_names column\n",
        "df['State_names'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9VsTcgZOhakP",
      "metadata": {
        "id": "9VsTcgZOhakP"
      },
      "source": [
        "#### Counts of unique values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec290b27",
      "metadata": {
        "id": "ec290b27"
      },
      "outputs": [],
      "source": [
        "df['Gender'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dVItecqDKUwk",
      "metadata": {
        "id": "dVItecqDKUwk"
      },
      "source": [
        "If we want to show with the percentage of occurrence rather number than we use `normalize=True` argument in `value_counts()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4876aa46",
      "metadata": {
        "id": "4876aa46"
      },
      "outputs": [],
      "source": [
        "# Calculate percentage of each category\n",
        "df['Gender'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665280f1",
      "metadata": {
        "id": "665280f1"
      },
      "outputs": [],
      "source": [
        "df['State_names'].value_counts().sort_values(ascending = False).head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed29623",
      "metadata": {
        "id": "eed29623"
      },
      "outputs": [],
      "source": [
        "# Sort Values by State_names\n",
        "df.sort_values(by=['State_names']).head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_4zUpaMCm6Ig",
      "metadata": {
        "id": "_4zUpaMCm6Ig"
      },
      "source": [
        "For sorting our dataframe by Amount_spent with ascending order:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9nAJo_njnDTr",
      "metadata": {
        "id": "9nAJo_njnDTr"
      },
      "outputs": [],
      "source": [
        "# Sort Values Amount_spent with ascending order\n",
        "df.sort_values(by=['Amount_spent']).head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r0GtAVPrp2BV",
      "metadata": {
        "id": "r0GtAVPrp2BV"
      },
      "source": [
        "For sorting our dataframe by Amount_spent with descending order:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GYRYIUCHpQU5",
      "metadata": {
        "id": "GYRYIUCHpQU5"
      },
      "outputs": [],
      "source": [
        "# Sort Values Amount_spent with descending order\n",
        "df.sort_values(by=['Age'], ascending=False).loc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TGbW_Qtmn1C2",
      "metadata": {
        "id": "TGbW_Qtmn1C2"
      },
      "source": [
        "Alternatively, We can use `nlargest()` and `nsmallest()` functions for displaying largest and smallest values with desired numbers. for example, If we want to display 4 largest Amount_spent rows then we use this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1580b9fe",
      "metadata": {
        "id": "1580b9fe"
      },
      "outputs": [],
      "source": [
        "# nlargest\n",
        "df.nlargest(4, 'Amount_spent').head(10) # first argument is how many rows you want to disply and second one is columns name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T0YVV84jrCXX",
      "metadata": {
        "id": "T0YVV84jrCXX"
      },
      "source": [
        "For 3 smallest Amount_spent rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458403e0",
      "metadata": {
        "id": "458403e0"
      },
      "outputs": [],
      "source": [
        "# nsmallest\n",
        "df.nsmallest(3, 'Age').head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LV6wnCzjwgfv",
      "metadata": {
        "id": "LV6wnCzjwgfv"
      },
      "source": [
        "#### Conditional queries on Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9998e789",
      "metadata": {
        "id": "9998e789"
      },
      "outputs": [],
      "source": [
        "# filtering - Only show Paypal users\n",
        "condition = df['Payment_method'] == 'PayPal'\n",
        "df[condition].head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nSlQRRNrwn8S",
      "metadata": {
        "id": "nSlQRRNrwn8S"
      },
      "source": [
        "We can apply multiple conditional queries like before. For example, if we want to display all Married female people who lived in New York then we use the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YvIpSZM2xiD4",
      "metadata": {
        "id": "YvIpSZM2xiD4"
      },
      "outputs": [],
      "source": [
        "# first create 3 condition\n",
        "female_person = df['Gender'] == 'Female'\n",
        "married_person = df['Marital_status'] == 'Married'\n",
        "loc_newyork = df['State_names'] == 'New York'\n",
        "\n",
        "# we passing condition on our dataframe\n",
        "df[female_person & married_person & loc_newyork].head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x0r-TXU244iu",
      "metadata": {
        "id": "x0r-TXU244iu"
      },
      "source": [
        "### 5.3 Summarizing or grouping data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NtTwyDD77VG0",
      "metadata": {
        "id": "NtTwyDD77VG0"
      },
      "source": [
        "#### Groupby\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2o84INb7AsFp",
      "metadata": {
        "id": "2o84INb7AsFp"
      },
      "source": [
        "**Grouping by one column:** For example, if we want to find `maximum` values of `Age` and `Amount_spent` by `Gender` then we can use this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2p_-CTIG-hSk",
      "metadata": {
        "id": "2p_-CTIG-hSk"
      },
      "outputs": [],
      "source": [
        "df[['Age', 'Amount_spent']].groupby(df['Gender']).max()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7WCCLbwfCy_q",
      "metadata": {
        "id": "7WCCLbwfCy_q"
      },
      "source": [
        "To find `mean`, `count`, and `max` values of `Age` and `Amount_spent` by `Gender` then we can use `agg()` function with `groupby()` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b47c9d9",
      "metadata": {
        "id": "2b47c9d9"
      },
      "outputs": [],
      "source": [
        "# Group by one columns\n",
        "state_gender_res = df[['Age','Gender','Amount_spent']].groupby(['Gender']).agg(['count', 'mean', 'max'])\n",
        "state_gender_res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9umxlA7YDjvD",
      "metadata": {
        "id": "9umxlA7YDjvD"
      },
      "source": [
        "**Grouping by multiple columns:** To find total count, maximum and minimum values of Amount_spent by State_names, Gender, and Payment_method then we can pass these columns names under `groupby()` function and add `.agg()` with `count`, `mean`, `max` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667947eb",
      "metadata": {
        "id": "667947eb"
      },
      "outputs": [],
      "source": [
        "#Group By multiple columns\n",
        "state_gender_res = df[['State_names','Gender','Payment_method','Amount_spent']].groupby([ 'State_names','Gender', 'Payment_method']).agg(['count', 'min', 'max'])\n",
        "state_gender_res.head(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BTEMA10_JfrV",
      "metadata": {
        "id": "BTEMA10_JfrV"
      },
      "source": [
        "#### Cross Tabulation (Crosstab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bNNkQ9-eRclS",
      "metadata": {
        "id": "bNNkQ9-eRclS"
      },
      "source": [
        "For creating a simple crosstab between Maritatal_status and Payment_method columns we just use `crosstab()` with both column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_9rixwbc5VEe",
      "metadata": {
        "id": "_9rixwbc5VEe"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df.Marital_status, df.Payment_method)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OK_gu5KFSETb",
      "metadata": {
        "id": "OK_gu5KFSETb"
      },
      "source": [
        "We can include subtotals by `margins` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UNpAxzbLSMBJ",
      "metadata": {
        "id": "UNpAxzbLSMBJ"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df.Marital_status, df.Payment_method, margins=True, margins_name=\"Total\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rDLBsIoZVRUK",
      "metadata": {
        "id": "rDLBsIoZVRUK"
      },
      "source": [
        "If We want a display with percentage than `normalize=True` parameter help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rM6o2CocVeYG",
      "metadata": {
        "id": "rM6o2CocVeYG"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df.Marital_status, df.Payment_method, normalize=True, margins=True, margins_name=\"Total\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D3GJPV8aVxsD",
      "metadata": {
        "id": "D3GJPV8aVxsD"
      },
      "source": [
        "In this crosstab features, we can pass multiple columns names for grouping and analyzing data. For instance, If we want to see how the `Payment_method` and `Employees_status` are distributed by `Marital_status` then we will pass these columns' names in `crosstab()` function and it will show below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uIHl15rL6fzm",
      "metadata": {
        "id": "uIHl15rL6fzm"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(df.Marital_status, [df.Payment_method, df.Employees_status])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c7fbc9c",
      "metadata": {
        "id": "2c7fbc9c"
      },
      "source": [
        "## 6. Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uOkFj7ixq9d2",
      "metadata": {
        "id": "uOkFj7ixq9d2"
      },
      "source": [
        "Visualization is the key to data analysis. The most popular python package for visualization are matplotlib and seaborn but sometimes pandas will be handy for you. Pandas also provide some visualization plots easily. For the basic analysis part, it will be easy to use. For this section, we are exploring some different types of plots using pandas. Here are the plots."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GeYsxRfly4Oq",
      "metadata": {
        "id": "GeYsxRfly4Oq"
      },
      "source": [
        "### 6.1 Line plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x4GV-jQS3Wf3",
      "metadata": {
        "id": "x4GV-jQS3Wf3"
      },
      "source": [
        "For creating a line plot in pandas we use `.plot()` with two columns name for the argument. For example, we create a line plot from one dummy dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ysTCzcKfHLU6"
      },
      "id": "ysTCzcKfHLU6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5uTSu01EmH-p",
      "metadata": {
        "id": "5uTSu01EmH-p"
      },
      "outputs": [],
      "source": [
        "dict_line = {\n",
        "    'year': [2016, 2017, 2018, 2019, 2020, 2021],\n",
        "    'price': [200, 250, 260, 220, 280, 300]\n",
        "}\n",
        "df_line = pd.DataFrame(dict_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q8Oo5OrpnSQe",
      "metadata": {
        "id": "Q8Oo5OrpnSQe"
      },
      "outputs": [],
      "source": [
        "# use plot() method on the dataframe\n",
        "df_line.plot('year', 'price');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setZs0vH35tj",
      "metadata": {
        "id": "setZs0vH35tj"
      },
      "source": [
        "The above line chart shows prices over a different time. It shows like price trend."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ki2MpNQ6bQUW",
      "metadata": {
        "id": "ki2MpNQ6bQUW"
      },
      "source": [
        "### 6.2 Bar plot\n",
        "\n",
        "A bar plot is also known as a bar chart shows quantitative or qualitative values for different category items. In a bar plot data are represented in the form of bars. Bars length or height are used to represent the quantitative value for each item. Bar plot can be plotted horizontally or vertically. For creating these plots look below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0o6W3oobIA9",
      "metadata": {
        "id": "c0o6W3oobIA9"
      },
      "source": [
        "**For horizontal bar:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ykQ0Vz8GZr63",
      "metadata": {
        "id": "ykQ0Vz8GZr63"
      },
      "outputs": [],
      "source": [
        "df['Employees_status'].value_counts().plot(kind='bar');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hUW2_IyhbNtQ",
      "metadata": {
        "id": "hUW2_IyhbNtQ"
      },
      "source": [
        "**For vertical bar:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0RFfg-TzbTB5",
      "metadata": {
        "id": "0RFfg-TzbTB5"
      },
      "outputs": [],
      "source": [
        "df['Employees_status'].value_counts().plot(kind='barh');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZdCUNgj9iKHo",
      "metadata": {
        "id": "ZdCUNgj9iKHo"
      },
      "source": [
        "### 6.3 Pie plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EG1XHZHLbiFX",
      "metadata": {
        "id": "EG1XHZHLbiFX"
      },
      "source": [
        "A pie plot is also known as a pie chart. A pie plot is a circular graph that represents the total value with its components. The area of a circle represents the total value and the different sectors of the circle represent the different parts. In this plot, the data are expressed as percentages. Each component is expressed as a percentage of the total value."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DYGE4SaNcZLo",
      "metadata": {
        "id": "DYGE4SaNcZLo"
      },
      "source": [
        "In pandas for creating pie plot. We use `kind=pie` in `plot()` function in dataframes columns or series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jf61Zg3GiPFp",
      "metadata": {
        "id": "jf61Zg3GiPFp"
      },
      "outputs": [],
      "source": [
        "df['Segment'].value_counts().plot(\n",
        "    kind='pie');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "utOuo_y6biXn",
      "metadata": {
        "id": "utOuo_y6biXn"
      },
      "source": [
        "### 6.4 Box Plot\n",
        "A box plot is also known as a box and whisker plot. This plot is used to show the distribution of a variable based on its quartiles. Box plot displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. It will also be popular to identify outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FK-M1zjEeWoB",
      "metadata": {
        "id": "FK-M1zjEeWoB"
      },
      "source": [
        "We can plot this by one column or multiple columns. For multiple columns, we need to pass columns name in `y` variable as a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oh9c2_0xblDZ",
      "metadata": {
        "id": "Oh9c2_0xblDZ"
      },
      "outputs": [],
      "source": [
        "df.plot(y=['Amount_spent'], kind='box');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LsLAK0twwF5h",
      "metadata": {
        "id": "LsLAK0twwF5h"
      },
      "source": [
        "In a boxplot, we can plot the distribution of categorical variables against a numerical variable and compare them. Let's plot it with the Employees_status and Amount_spent columns with pandas `boxplot()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CMdCBcUjtY85",
      "metadata": {
        "id": "CMdCBcUjtY85"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "\n",
        "df.boxplot(by ='Employees_status', column =['Amount_spent'],ax=ax, grid = False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XlioEpoHf5tx",
      "metadata": {
        "id": "XlioEpoHf5tx"
      },
      "source": [
        "### 6.5 Histogram\n",
        "\n",
        "A histogram shows the frequency and distribution of quantitative measurement across grouped values for data items. It is commonly used in statistics to show how many of a certain type of variable occurs within a specific range or bucket. Below we will plot a histogram for looking Age distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tIfoa79re8ca",
      "metadata": {
        "id": "tIfoa79re8ca"
      },
      "outputs": [],
      "source": [
        "df.plot(\n",
        "    y='Age',\n",
        "    kind='hist',\n",
        "    bins=10\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zlw8AfaLgCIp",
      "metadata": {
        "id": "zlw8AfaLgCIp"
      },
      "source": [
        "### 6.6 KDE plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bsv4rB3rh9kY",
      "metadata": {
        "id": "bsv4rB3rh9kY"
      },
      "source": [
        "A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YW8XyrPugFnL",
      "metadata": {
        "id": "YW8XyrPugFnL"
      },
      "outputs": [],
      "source": [
        "df.plot(\n",
        "    y='Age',\n",
        "    xlim=(0, 100),\n",
        "    kind='kde'\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BDnue_BWhLmF",
      "metadata": {
        "id": "BDnue_BWhLmF"
      },
      "source": [
        "### 6.7 Scatterplot\n",
        "A scatterplot is used to observe and show relationships between two quantitative variables for different category items. Each member of the dataset gets plotted as a point whose x-y coordinates relate to its values for the two variables. Below we will plot a scatterplot to display relationships between Age and Amount_spent columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zF0yBKd8hObb",
      "metadata": {
        "id": "zF0yBKd8hObb"
      },
      "outputs": [],
      "source": [
        "df.plot(\n",
        "    x='Age',\n",
        "    y='Amount_spent',\n",
        "    kind='scatter'\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9414a18",
      "metadata": {
        "id": "a9414a18"
      },
      "source": [
        "## 7. Reference\n",
        "\n",
        "\n",
        "1. [Pandas user guide](https://pandas.pydata.org/docs/user_guide/index.html)\n",
        "2. [Pandas 1.x Cookbook](https://www.packtpub.com/product/pandas-1-x-cookbook-second-edition/9781839213106)\n",
        "3. [The Data Wrangling Workshop](https://www.packtpub.com/product/the-data-wrangling-workshop-second-edition/9781839215001)\n",
        "4. [Python for Data Analysis](https://www.oreilly.com/library/view/python-for-data/9781449323592/)\n",
        "5. [Data Analysis with Python: Zero to Pandas - Jovian YouTube Channel](https://www.youtube.com/watch?v=BaV4PRXYNIY&list=PLyMom0n-MBrpzC91Uo560S4VbsiLYtCwo)\n",
        "6. [Best practices with pandas - Data School YouTube Channel](https://www.youtube.com/watch?v=hl-TGI4550M&list=PL5-da3qGB5IBITZj_dYSFqnd_15JgqwA6)\n",
        "7. [Pandas Tutorials - Corey Schafer YouTube Channel](https://www.youtube.com/watch?v=ZyhVh-qRZPA&list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS)\n",
        "8. [Pandas Crosstab Explained](https://pbpython.com/pandas-crosstab.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "q72dXwaog4h7",
        "68cb06b1",
        "2098e35f",
        "44ff83e8",
        "5UgQzwrITZk3",
        "6bxAq9hnUK4R",
        "2980dcd0",
        "QyO24xKLTEEm",
        "YC_ucd2fXzjD",
        "d9d6f71f",
        "4e3b208b",
        "CZnzwDXQg35f",
        "x0r-TXU244iu",
        "2c7fbc9c"
      ],
      "name": "pandas_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}